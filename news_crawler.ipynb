{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> imports <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, yaml\n",
    "\n",
    "import time, threading, re, random\n",
    "from collections import defaultdict\n",
    "import urllib.parse as urlparse\n",
    "import requests\n",
    "from urllib.robotparser import RobotFileParser\n",
    "import tldextract\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 1.\tCrawler / Scraper <h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining basic settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configs/crawl_config.yaml saved\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"configs\", exist_ok=True)\n",
    "os.makedirs(\"storage/raw\", exist_ok=True)\n",
    "os.makedirs(\"storage/clean\", exist_ok=True)\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "CONFIG = {\n",
    "    \"user_agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36\",\n",
    "    \"default_timeout_sec\": 15,\n",
    "    \"per_host_min_delay_sec\": 2,\n",
    "    \"max_retries\": 3,\n",
    "    \"sources\": {\n",
    "        \"reddit\": {\n",
    "            \"type\": \"reddit_html\",\n",
    "            \"subreddits\": [\"worldnews\", \"news\"],\n",
    "            \"limit_per_sub\": 20\n",
    "        },\n",
    "        \"news_sites\": [\n",
    "            {\n",
    "                \"name\": \"reuters_world\",\n",
    "                \"start_urls\": [\"https://www.reuters.com/world/\"],\n",
    "                \"article_selector_hint\": \"a\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(\"configs/crawl_config.yaml\", \"w\", encoding=\"utf-8\") as f:\n",
    "    yaml.safe_dump(CONFIG, f, allow_unicode=True)\n",
    "print(\"configs/crawl_config.yaml saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the robots.txt and control the rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "_session = requests.Session()\n",
    "# _session.headers.update({\"Accept-Language\": \"en;q=0.9\"})\n",
    "_session.headers.update({\n",
    "    \"Accept-Language\": \"en;q=0.9\",\n",
    "    \"User-Agent\": CONFIG[\"user_agent\"],   # اضافه شود\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "    \"Connection\": \"keep-alive\"\n",
    "})\n",
    "\n",
    "_robot_cache = {}\n",
    "_host_next_time = defaultdict(float)\n",
    "_lock = threading.Lock()\n",
    "\n",
    "def get_robots_parser(base_url, ua):\n",
    "    parsed = urlparse.urlparse(base_url)\n",
    "    robots_url = f\"{parsed.scheme}://{parsed.netloc}/robots.txt\"\n",
    "    if robots_url in _robot_cache:\n",
    "        return _robot_cache[robots_url]\n",
    "    rp = RobotFileParser()\n",
    "    try:\n",
    "        resp = _session.get(robots_url, timeout=8)\n",
    "        if resp.status_code == 200:\n",
    "            rp.parse(resp.text.splitlines())\n",
    "            print(\"resp.status_code == 200\")\n",
    "        else:\n",
    "            rp.parse([])\n",
    "            print(\"resp.status_code != 200\")\n",
    "    except Exception:\n",
    "        rp.parse([])\n",
    "    _robot_cache[robots_url] = rp\n",
    "    return rp\n",
    "\n",
    "def host_key(url):\n",
    "    p = urlparse.urlparse(url)\n",
    "    ext = tldextract.extract(p.netloc)\n",
    "    return \".\".join([x for x in [ext.domain, ext.suffix] if x])\n",
    "\n",
    "def rate_limit(url, min_delay_sec):\n",
    "    hk = host_key(url)\n",
    "    with _lock:\n",
    "        now = time.time()\n",
    "        nt = _host_next_time[hk]\n",
    "        wait = nt - now\n",
    "        if wait > 0:\n",
    "            time.sleep(wait)\n",
    "        _host_next_time[hk] = time.time() + min_delay_sec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch function (with Retry & Exponential Backoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch(url, user_agent, timeout=15, max_retries=3, min_delay_sec=2):\n",
    "    rp = get_robots_parser(url, user_agent)\n",
    "    if not rp.can_fetch(user_agent, url):\n",
    "        raise PermissionError(f\"Blocked by robots.txt for {url}\")\n",
    "    rate_limit(url, min_delay_sec)\n",
    "    headers = {\"User-Agent\": user_agent}\n",
    "    attempt = 0\n",
    "    backoff = 1.6\n",
    "    while attempt <= max_retries:\n",
    "        try:\n",
    "            resp = _session.get(url, headers=headers, timeout=timeout)\n",
    "            if 200 <= resp.status_code < 300:\n",
    "                return resp\n",
    "            if resp.status_code in (429, 503):\n",
    "                time.sleep((backoff ** attempt) + random.uniform(0, 0.5))\n",
    "            else:\n",
    "                time.sleep(0.6)\n",
    "        except requests.RequestException:\n",
    "            time.sleep((backoff ** attempt) + 0.4)\n",
    "        attempt += 1\n",
    "    raise TimeoutError(f\"Fetch failed after retries for {url}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML Crawler for Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_reddit_listing(html, base_url):\n",
    "#     soup = BeautifulSoup(html, \"lxml\")\n",
    "#     items = []\n",
    "#     # Generic selector for post container\n",
    "#     for post in soup.select(\"[data-testid='post-container']\"):\n",
    "#         title_el = post.select_one(\"h3\")\n",
    "#         if not title_el:\n",
    "#             continue\n",
    "#         title = title_el.get_text(strip=True)\n",
    "#         link_el = post.find(\"a\", href=True)\n",
    "#         url = urlparse.urljoin(base_url, link_el[\"href\"]) if link_el else None\n",
    "\n",
    "#         items.append({\n",
    "#             \"source_type\": \"reddit\",\n",
    "#             \"source_name\": base_url,\n",
    "#             \"subreddit\": base_url.rstrip(\"/\").split(\"/\")[-1],\n",
    "#             \"url\": url,\n",
    "#             \"canonical_url\": url,\n",
    "#             \"title\": title,\n",
    "#             \"text\": None,\n",
    "#             \"author\": None,\n",
    "#             \"published_at\": None,\n",
    "#             \"score\": None,\n",
    "#             \"comments\": None,\n",
    "#             \"fetched_at\": datetime.utcnow().isoformat(timespec=\"seconds\")\n",
    "#         })\n",
    "#     return items\n",
    "\n",
    "# def crawl_reddit_subreddit(sub, cfg):\n",
    "#     ua = cfg[\"user_agent\"]\n",
    "#     limit = cfg[\"sources\"][\"reddit\"][\"limit_per_sub\"]\n",
    "#     url = f\"https://www.reddit.com/r/{sub}/\"\n",
    "#     resp = fetch(\n",
    "#         url, ua, timeout=cfg[\"default_timeout_sec\"], \n",
    "#         max_retries=cfg[\"max_retries\"], \n",
    "#         min_delay_sec=cfg[\"per_host_min_delay_sec\"]\n",
    "#     )\n",
    "#     items = parse_reddit_listing(resp.text, url)\n",
    "#     return items[:limit] if limit else items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import urllib.parse as urlparse\n",
    "\n",
    "UPVOTE_RE = re.compile(r\"(\\d+(?:\\.\\d+)?)([kK])?\\s*upvote\")  # برای استخراج عدد از aria-label\n",
    "\n",
    "def to_int_k(v):\n",
    "    if v is None:\n",
    "        return None\n",
    "    s = str(v).strip().lower()\n",
    "    if s.endswith(\"k\"):\n",
    "        try:\n",
    "            return int(float(s[:-1]) * 1000)\n",
    "        except:\n",
    "            return None\n",
    "    try:\n",
    "        return int(s)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parse_reddit_listing(html, base_url):\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    items = []\n",
    "\n",
    "    for post in soup.select(\"[data-testid='post-container'], div[data-test-id='post-content']\"):\n",
    "        # عنوان\n",
    "        title_el = post.select_one(\"h3\") or post.select_one(\"a[data-click-id='body'] h3\")\n",
    "        if not title_el:\n",
    "            continue\n",
    "        title = title_el.get_text(strip=True)\n",
    "\n",
    "        # لینک پایدار پست\n",
    "        link_el = post.select_one(\"a[data-click-id='body']\") or post.find(\"a\", href=True)\n",
    "        url = urlparse.urljoin(base_url, link_el[\"href\"]) if link_el and link_el.get(\"href\") else None\n",
    "\n",
    "        # نویسنده\n",
    "        author_el = post.select_one(\"a[data-click-id='user']\") or post.select_one(\"a[href^='/user/']\")\n",
    "        author = author_el.get_text(strip=True) if author_el else None\n",
    "\n",
    "        # زمان انتشار\n",
    "        time_el = post.select_one(\"a[data-click-id='timestamp'] time\") or post.find(\"time\")\n",
    "        published_at = time_el.get(\"datetime\") if time_el and time_el.has_attr(\"datetime\") else None\n",
    "\n",
    "        # شمار نظرها\n",
    "        comments_el = post.select_one(\"a[data-click-id='comments']\") or post.find(\"a\", string=re.compile(\"comment\", re.I))\n",
    "        comments = None\n",
    "        if comments_el:\n",
    "            m = re.search(r\"(\\d+(?:\\.\\d+)?[kK]?)\", comments_el.get_text(\" \", strip=True))\n",
    "            if m:\n",
    "                comments = to_int_k(m.group(1))\n",
    "\n",
    "        # امتیاز تقریبی\n",
    "        score = None\n",
    "        aria_up = post.find(attrs={\"aria-label\": re.compile(\"upvote\", re.I)})\n",
    "        if aria_up:\n",
    "            m = UPVOTE_RE.search(aria_up.get(\"aria-label\", \"\"))\n",
    "            if m:\n",
    "                val = float(m.group(1))\n",
    "                if m.group(2):\n",
    "                    val *= 1000\n",
    "                score = int(val)\n",
    "\n",
    "        items.append({\n",
    "            \"source_type\": \"reddit\",\n",
    "            \"source_name\": base_url,\n",
    "            \"subreddit\": base_url.rstrip(\"/\").split(\"/\")[-1],\n",
    "            \"url\": url,\n",
    "            \"canonical_url\": url,  # بعدا در مرحله کاننیکال اصلاح می‌کنیم\n",
    "            \"title\": title,\n",
    "            \"text\": None,\n",
    "            \"author\": author,\n",
    "            \"published_at\": published_at,\n",
    "            \"score\": score,\n",
    "            \"comments\": comments,\n",
    "            \"fetched_at\": datetime.utcnow().isoformat(timespec=\"seconds\")\n",
    "        })\n",
    "    return items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from hashlib import sha1\n",
    "\n",
    "# STRIP_PARAMS = {\"utm_source\",\"utm_medium\",\"utm_campaign\",\"utm_term\",\"utm_content\",\"ref\",\"utm_name\"}\n",
    "\n",
    "# def normalize_url(u):\n",
    "#     if not u:\n",
    "#         return None\n",
    "#     p = urlparse.urlparse(u)\n",
    "#     # حذف پارامترهای رهگیری\n",
    "#     q = urlparse.parse_qsl(p.query, keep_blank_values=True)\n",
    "#     q = [(k,v) for k,v in q if k not in STRIP_PARAMS]\n",
    "#     new_q = urlparse.urlencode(q)\n",
    "#     norm = urlparse.urlunparse((p.scheme, p.netloc, p.path, \"\", new_q, \"\"))\n",
    "#     return norm\n",
    "\n",
    "# def get_canonical_from_html(html, url):\n",
    "#     soup = BeautifulSoup(html, \"lxml\")\n",
    "#     link = soup.find(\"link\", rel=lambda x: x and \"canonical\" in x.lower())\n",
    "#     if link and link.get(\"href\"):\n",
    "#         return urlparse.urljoin(url, link[\"href\"])\n",
    "#     # پشتیبان، اگر داده ساخت یافته وجود داشت\n",
    "#     meta = soup.find(\"meta\", property=\"og:url\")\n",
    "#     if meta and meta.get(\"content\"):\n",
    "#         return urlparse.urljoin(url, meta[\"content\"])\n",
    "#     return None\n",
    "\n",
    "# def dedupe_records(records):\n",
    "#     seen = set()\n",
    "#     unique = []\n",
    "#     for r in records:\n",
    "#         key = normalize_url(r.get(\"canonical_url\") or r.get(\"url\")) or r.get(\"url\")\n",
    "#         if not key:\n",
    "#             continue\n",
    "#         h = sha1(key.encode(\"utf-8\")).hexdigest()\n",
    "#         if h in seen:\n",
    "#             continue\n",
    "#         seen.add(h)\n",
    "#         r[\"canonical_url\"] = key\n",
    "#         unique.append(r)\n",
    "#     return unique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== helpers for canonicalization and meta extraction ====\n",
    "import re\n",
    "from hashlib import sha1\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse as urlparse\n",
    "\n",
    "STRIP_PARAMS = {\"utm_source\",\"utm_medium\",\"utm_campaign\",\"utm_term\",\"utm_content\",\"ref\",\"utm_name\",\"gclid\",\"fbclid\"}\n",
    "\n",
    "def normalize_url(u: str) -> str | None:\n",
    "    if not u:\n",
    "        return None\n",
    "    p = urlparse.urlparse(u)\n",
    "    if p.scheme not in (\"http\",\"https\"):\n",
    "        return None\n",
    "    q = [(k,v) for k,v in urlparse.parse_qsl(p.query, keep_blank_values=True) if k not in STRIP_PARAMS]\n",
    "    new_q = urlparse.urlencode(q)\n",
    "    return urlparse.urlunparse((p.scheme, p.netloc, p.path, \"\", new_q, \"\"))\n",
    "\n",
    "def get_canonical_from_html(html: str, url: str) -> str | None:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    link = soup.find(\"link\", rel=lambda x: x and \"canonical\" in x.lower())\n",
    "    if link and link.get(\"href\"):\n",
    "        return urlparse.urljoin(url, link[\"href\"])\n",
    "    meta = soup.find(\"meta\", property=\"og:url\")\n",
    "    if meta and meta.get(\"content\"):\n",
    "        return urlparse.urljoin(url, meta[\"content\"])\n",
    "    return None\n",
    "\n",
    "def extract_article_meta(html: str, url: str):\n",
    "    \"\"\"برگشت می دهد title و author و published_at و canonical\"\"\"\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    # title\n",
    "    title = None\n",
    "    for sel in [\n",
    "        \"meta[property='og:title']\",\n",
    "        \"meta[name='twitter:title']\",\n",
    "        \"title\"\n",
    "    ]:\n",
    "        el = soup.select_one(sel)\n",
    "        if el:\n",
    "            title = el.get(\"content\") if el.has_attr(\"content\") else el.get_text(strip=True)\n",
    "        if title:\n",
    "            break\n",
    "\n",
    "    # author\n",
    "    author = None\n",
    "    for sel in [\n",
    "        \"meta[name='author']\",\n",
    "        \"meta[property='article:author']\",\n",
    "        \"a[rel='author']\"\n",
    "    ]:\n",
    "        el = soup.select_one(sel)\n",
    "        if el:\n",
    "            author = el.get(\"content\") if el.has_attr(\"content\") else el.get_text(strip=True)\n",
    "        if author:\n",
    "            break\n",
    "\n",
    "    # published time\n",
    "    published_at = None\n",
    "    for sel in [\n",
    "        \"meta[property='article:published_time']\",\n",
    "        \"meta[name='pubdate']\",\n",
    "        \"time[datetime]\"\n",
    "    ]:\n",
    "        el = soup.select_one(sel)\n",
    "        if el:\n",
    "            published_at = el.get(\"content\") if el.has_attr(\"content\") else el.get(\"datetime\")\n",
    "        if published_at:\n",
    "            break\n",
    "\n",
    "    # canonical\n",
    "    can = get_canonical_from_html(html, url) or url\n",
    "    can = normalize_url(can)\n",
    "\n",
    "    return title, author, published_at, can\n",
    "\n",
    "def dedupe_records(records):\n",
    "    seen = set()\n",
    "    unique = []\n",
    "    for r in records:\n",
    "        key = normalize_url(r.get(\"canonical_url\") or r.get(\"url\")) or r.get(\"url\")\n",
    "        if not key:\n",
    "            continue\n",
    "        h = sha1(key.encode(\"utf-8\")).hexdigest()\n",
    "        if h in seen:\n",
    "            continue\n",
    "        seen.add(h)\n",
    "        r[\"canonical_url\"] = key\n",
    "        unique.append(r)\n",
    "    return unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_canonical_for_some(items, cfg, take=10):\n",
    "    ua = cfg[\"user_agent\"]\n",
    "    for i, r in enumerate(items[:take]):\n",
    "        if not r.get(\"url\"):\n",
    "            continue\n",
    "        try:\n",
    "            resp = fetch(r[\"url\"], ua, timeout=cfg[\"default_timeout_sec\"], \n",
    "                         max_retries=cfg[\"max_retries\"], \n",
    "                         min_delay_sec=cfg[\"per_host_min_delay_sec\"])\n",
    "            can = get_canonical_from_html(resp.text, r[\"url\"])\n",
    "            if can:\n",
    "                r[\"canonical_url\"] = normalize_url(can)\n",
    "            else:\n",
    "                r[\"canonical_url\"] = normalize_url(r[\"url\"])\n",
    "        except Exception:\n",
    "            r[\"canonical_url\"] = normalize_url(r[\"url\"])\n",
    "    # بقیه موارد حداقل نرمالایز شوند\n",
    "    for r in items[take:]:\n",
    "        r[\"canonical_url\"] = normalize_url(r.get(\"url\"))\n",
    "    return items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "\n",
    "def reddit_rss_url(sub: str) -> str:\n",
    "    # old.reddit.com و www.reddit.com هر دو RSS می‌دهند\n",
    "    return f\"https://old.reddit.com/r/{sub}/.rss\"\n",
    "\n",
    "def crawl_reddit_via_rss(sub, cfg, limit=None):\n",
    "    url = reddit_rss_url(sub)\n",
    "    d = feedparser.parse(url)\n",
    "    items = []\n",
    "    now = datetime.utcnow().isoformat(timespec=\"seconds\")\n",
    "    for e in d.get(\"entries\", [])[: (limit or 50)]:\n",
    "        link = e.get(\"link\")\n",
    "        title = e.get(\"title\")\n",
    "        author = None\n",
    "        if \"author\" in e:\n",
    "            author = e[\"author\"]\n",
    "        published_at = None\n",
    "        if \"published\" in e:\n",
    "            published_at = e[\"published\"]\n",
    "        items.append({\n",
    "            \"source_type\": \"reddit\",\n",
    "            \"source_name\": url,\n",
    "            \"subreddit\": sub,\n",
    "            \"url\": link,\n",
    "            \"canonical_url\": normalize_url(link) if link else link,\n",
    "            \"title\": title,\n",
    "            \"text\": None,\n",
    "            \"author\": author,\n",
    "            \"published_at\": published_at,\n",
    "            \"score\": None,\n",
    "            \"comments\": None,\n",
    "            \"fetched_at\": now\n",
    "        })\n",
    "    # dedupe نهایی\n",
    "    return dedupe_records(items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def crawl_reddit_subreddit(sub, cfg):\n",
    "#     ua = cfg[\"user_agent\"]\n",
    "#     limit = cfg[\"sources\"][\"reddit\"][\"limit_per_sub\"]\n",
    "#     # url = f\"https://www.reddit.com/r/{sub}/\"\n",
    "#     # از old.reddit.com استفاده \n",
    "#     url = f\"https://old.reddit.com/r/{sub}/\"\n",
    "#     resp = fetch(url, ua, timeout=cfg[\"default_timeout_sec\"], \n",
    "#                  max_retries=cfg[\"max_retries\"], \n",
    "#                  min_delay_sec=cfg[\"per_host_min_delay_sec\"])\n",
    "#     items = parse_reddit_listing(resp.text, url)\n",
    "\n",
    "#     # خواندن کاننیکال برای چند مورد اول\n",
    "#     items = enrich_canonical_for_some(items, cfg, take=10)\n",
    "#     # حذف تکراری\n",
    "#     items = dedupe_records(items)\n",
    "\n",
    "#     return items[:limit] if limit else items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_reddit_subreddit(sub, cfg):\n",
    "    ua = cfg[\"user_agent\"]\n",
    "    limit = cfg[\"sources\"][\"reddit\"][\"limit_per_sub\"]\n",
    "    html_url = f\"https://old.reddit.com/r/{sub}/\"\n",
    "\n",
    "    # اگر robots اجازه نداد به RSS سوییچ کن\n",
    "    rp = get_robots_parser(html_url, ua)\n",
    "    if not rp.can_fetch(ua, html_url):\n",
    "        return crawl_reddit_via_rss(sub, cfg, limit)\n",
    "\n",
    "    # در غیر این صورت HTML\n",
    "    resp = fetch(html_url, ua,\n",
    "                 timeout=cfg[\"default_timeout_sec\"],\n",
    "                 max_retries=cfg[\"max_retries\"],\n",
    "                 min_delay_sec=cfg[\"per_host_min_delay_sec\"])\n",
    "    items = parse_reddit_listing(resp.text, html_url)\n",
    "    items = enrich_canonical_for_some(items, cfg, take=10)\n",
    "    items = dedupe_records(items)\n",
    "    return items[:limit] if limit else items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def looks_like_consent_page(html: str) -> bool:\n",
    "    s = html.lower()\n",
    "    return (\"consent\" in s) or (\"gdpr\" in s) or (\"privacy preferences\" in s) or (\"iab\" in s)\n",
    "\n",
    "def crawl_news_site(entry, cfg, max_article_per_listing=30, min_text_len=300):\n",
    "    ua = cfg[\"user_agent\"]\n",
    "    start_urls = entry[\"start_urls\"]\n",
    "    selector_hint = entry.get(\"article_selector_hint\", \"a\")\n",
    "    records = []\n",
    "    seen = set()\n",
    "\n",
    "    for su in start_urls:\n",
    "        resp = fetch(su, ua,\n",
    "                     timeout=cfg[\"default_timeout_sec\"],\n",
    "                     max_retries=cfg[\"max_retries\"],\n",
    "                     min_delay_sec=cfg[\"per_host_min_delay_sec\"])\n",
    "        links = parse_listing_find_links(resp.text, su, selector_hint)\n",
    "\n",
    "        for lk in links[:max_article_per_listing]:\n",
    "            try:\n",
    "                art = fetch(lk, ua,\n",
    "                            timeout=cfg[\"default_timeout_sec\"],\n",
    "                            max_retries=cfg[\"max_retries\"],\n",
    "                            min_delay_sec=cfg[\"per_host_min_delay_sec\"])\n",
    "\n",
    "                # اگر صفحه consent بود از آن عبور کن\n",
    "                if looks_like_consent_page(art.text):\n",
    "                    print(\"skip consent page:\", lk)\n",
    "                    continue\n",
    "\n",
    "                text = extract_article_text(art.text) or \"\"\n",
    "                if len(text) < min_text_len:\n",
    "                    continue\n",
    "\n",
    "                title, author, published_at, canonical = extract_article_meta(art.text, lk)\n",
    "                canonical = canonical or normalize_url(lk)\n",
    "                if not canonical:\n",
    "                    continue\n",
    "                h = sha1(canonical.encode(\"utf-8\")).hexdigest()\n",
    "                if h in seen:\n",
    "                    continue\n",
    "                seen.add(h)\n",
    "\n",
    "                records.append({\n",
    "                    \"source_type\": \"news\",\n",
    "                    \"source_name\": entry[\"name\"],\n",
    "                    \"subreddit\": None,\n",
    "                    \"url\": lk,\n",
    "                    \"canonical_url\": canonical,\n",
    "                    \"title\": title,\n",
    "                    \"text\": text,\n",
    "                    \"author\": author,\n",
    "                    \"published_at\": published_at,\n",
    "                    \"score\": None,\n",
    "                    \"comments\": None,\n",
    "                    \"fetched_at\": datetime.utcnow().isoformat(timespec=\"seconds\")\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(\"error on\", lk, e)\n",
    "    return records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def looks_like_consent_page(html: str) -> bool:\n",
    "#     s = html.lower()\n",
    "#     return (\"consent\" in s) or (\"gdpr\" in s) or (\"privacy preferences\" in s) or (\"iab\" in s)\n",
    "\n",
    "# # داخل crawl_news_site پس از fetch هر مقاله\n",
    "# art = fetch(lk, ua,\n",
    "#             timeout=cfg[\"default_timeout_sec\"],\n",
    "#             max_retries=cfg[\"max_retries\"],\n",
    "#             min_delay_sec=cfg[\"per_host_min_delay_sec\"])\n",
    "# if looks_like_consent_page(art.text):\n",
    "#     continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def safe_print(*args):\n",
    "#     try:\n",
    "#         print(*args)\n",
    "#     except Exception:\n",
    "#         pass\n",
    "\n",
    "# # نمونه استفاده\n",
    "# if not rp.can_fetch(ua, html_url):\n",
    "#     safe_print(f\"robots disallows HTML for {html_url}, switching to RSS\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from readability import Document\n",
    "\n",
    "# def extract_article_text(html):\n",
    "#     try:\n",
    "#         doc = Document(html)\n",
    "#         content_html = doc.summary()\n",
    "#         soup = BeautifulSoup(content_html, \"lxml\")\n",
    "#         text = soup.get_text(\"\\n\", strip=True)\n",
    "#         if len(text) < 200:\n",
    "#             # fallback\n",
    "#             soup_full = BeautifulSoup(html, \"lxml\")\n",
    "#             paras = [p.get_text(\" \", strip=True) for p in soup_full.select(\"p\")]\n",
    "#             text = \"\\n\".join(paras[:60])\n",
    "#         return text\n",
    "#     except Exception:\n",
    "#         soup = BeautifulSoup(html, \"lxml\")\n",
    "#         paras = [p.get_text(\" \", strip=True) for p in soup.select(\"p\")]\n",
    "#         return \"\\n\".join(paras[:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== article text extraction (بدون تغییر اساسی) ====\n",
    "from readability import Document\n",
    "\n",
    "def extract_article_text(html):\n",
    "    try:\n",
    "        doc = Document(html)\n",
    "        content_html = doc.summary()\n",
    "        soup = BeautifulSoup(content_html, \"lxml\")\n",
    "        text = soup.get_text(\"\\n\", strip=True)\n",
    "        if len(text) < 200:\n",
    "            soup_full = BeautifulSoup(html, \"lxml\")\n",
    "            paras = [p.get_text(\" \", strip=True) for p in soup_full.select(\"p\")]\n",
    "            text = \"\\n\".join(paras[:60])\n",
    "        return text\n",
    "    except Exception:\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        paras = [p.get_text(\" \", strip=True) for p in soup.select(\"p\")]\n",
    "        return \"\\n\".join(paras[:60])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_listing_find_links(html, base_url, selector_hint=\"a\"):\n",
    "#     soup = BeautifulSoup(html, \"lxml\")\n",
    "#     links = []\n",
    "#     for a in soup.select(selector_hint):\n",
    "#         href = a.get(\"href\")\n",
    "#         if not href:\n",
    "#             continue\n",
    "#         full = urlparse.urljoin(base_url, href)\n",
    "#         links.append(full)\n",
    "#     # حذف تکراری\n",
    "#     return list(dict.fromkeys(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== smarter link discovery on listing pages ====\n",
    "def parse_listing_find_links(html, base_url, selector_hint=\"a\"):\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    base = urlparse.urlparse(base_url).netloc\n",
    "    links = []\n",
    "    for a in soup.select(selector_hint):\n",
    "        href = a.get(\"href\")\n",
    "        if not href:\n",
    "            continue\n",
    "        full = urlparse.urljoin(base_url, href)\n",
    "        norm = normalize_url(full)\n",
    "        if not norm:\n",
    "            continue\n",
    "        # only same-site pages, skip anchors and mailto and javascript\n",
    "        net = urlparse.urlparse(norm).netloc\n",
    "        if net != base:\n",
    "            continue\n",
    "        if norm.endswith(\"#\"):\n",
    "            norm = norm[:-1]\n",
    "        links.append(norm)\n",
    "    # unique while keeping order\n",
    "    return list(dict.fromkeys(links))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def crawl_news_site(entry, cfg):\n",
    "#     ua = cfg[\"user_agent\"]\n",
    "#     start_urls = entry[\"start_urls\"]\n",
    "#     selector_hint = entry.get(\"article_selector_hint\", \"a\")\n",
    "#     records = []\n",
    "#     for su in start_urls:\n",
    "#         resp = fetch(su, ua, timeout=cfg[\"default_timeout_sec\"], max_retries=cfg[\"max_retries\"], min_delay_sec=cfg[\"per_host_min_delay_sec\"])\n",
    "#         links = parse_listing_find_links(resp.text, su, selector_hint)\n",
    "#         for lk in links[:30]:\n",
    "#             try:\n",
    "#                 art = fetch(lk, ua, timeout=cfg[\"default_timeout_sec\"], max_retries=cfg[\"max_retries\"], min_delay_sec=cfg[\"per_host_min_delay_sec\"])\n",
    "#                 text = extract_article_text(art.text)\n",
    "#                 records.append({\n",
    "#                     \"source_type\": \"news\",\n",
    "#                     \"source_name\": entry[\"name\"],\n",
    "#                     \"subreddit\": None,\n",
    "#                     \"url\": lk,\n",
    "#                     \"canonical_url\": lk,\n",
    "#                     \"title\": None,\n",
    "#                     \"text\": text,\n",
    "#                     \"author\": None,\n",
    "#                     \"published_at\": None,\n",
    "#                     \"score\": None,\n",
    "#                     \"comments\": None,\n",
    "#                     \"fetched_at\": datetime.utcnow().isoformat(timespec=\"seconds\")\n",
    "#                 })\n",
    "#             except Exception as e:\n",
    "#                 print(\"error on\", lk, e)\n",
    "#     return records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== crawl a news site with meta, canonical and dedupe ====\n",
    "from datetime import datetime\n",
    "\n",
    "def crawl_news_site(entry, cfg, max_article_per_listing=30, min_text_len=300):\n",
    "    ua = cfg[\"user_agent\"]\n",
    "    start_urls = entry[\"start_urls\"]\n",
    "    selector_hint = entry.get(\"article_selector_hint\", \"a\")\n",
    "    records = []\n",
    "    seen = set()  # dedupe on canonical\n",
    "\n",
    "    for su in start_urls:\n",
    "        resp = fetch(su, ua,\n",
    "                     timeout=cfg[\"default_timeout_sec\"],\n",
    "                     max_retries=cfg[\"max_retries\"],\n",
    "                     min_delay_sec=cfg[\"per_host_min_delay_sec\"])\n",
    "        links = parse_listing_find_links(resp.text, su, selector_hint)\n",
    "\n",
    "        for lk in links[:max_article_per_listing]:\n",
    "            try:\n",
    "                art = fetch(lk, ua,\n",
    "                            timeout=cfg[\"default_timeout_sec\"],\n",
    "                            max_retries=cfg[\"max_retries\"],\n",
    "                            min_delay_sec=cfg[\"per_host_min_delay_sec\"])\n",
    "                text = extract_article_text(art.text) or \"\"\n",
    "                if len(text) < min_text_len:\n",
    "                    continue  # likely not an article\n",
    "\n",
    "                title, author, published_at, canonical = extract_article_meta(art.text, lk)\n",
    "                canonical = canonical or normalize_url(lk)\n",
    "                if not canonical:\n",
    "                    continue\n",
    "                h = sha1(canonical.encode(\"utf-8\")).hexdigest()\n",
    "                if h in seen:\n",
    "                    continue\n",
    "                seen.add(h)\n",
    "\n",
    "                records.append({\n",
    "                    \"source_type\": \"news\",\n",
    "                    \"source_name\": entry[\"name\"],\n",
    "                    \"subreddit\": None,\n",
    "                    \"url\": lk,\n",
    "                    \"canonical_url\": canonical,\n",
    "                    \"title\": title,\n",
    "                    \"text\": text,\n",
    "                    \"author\": author,\n",
    "                    \"published_at\": published_at,\n",
    "                    \"score\": None,\n",
    "                    \"comments\": None,\n",
    "                    \"fetched_at\": datetime.utcnow().isoformat(timespec=\"seconds\")\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(\"error on\", lk, e)\n",
    "    return records\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSV schema & saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_SCHEMA = [\n",
    "    \"source_type\", \"source_name\", \"subreddit\", \"url\", \"canonical_url\",\n",
    "    \"title\", \"text\", \"author\", \"published_at\", \"score\", \"comments\",\n",
    "    \"language\", \"token_count\", \"predicted_label\", \"label_scores\", \"fetched_at\"\n",
    "]\n",
    "\n",
    "def to_dataframe(records):\n",
    "    df = pd.DataFrame(records)\n",
    "    for col in CSV_SCHEMA:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "    return df[CSV_SCHEMA]\n",
    "\n",
    "def save_csv(df, path):\n",
    "    df.to_csv(path, index=False, encoding=\"utf-8\")\n",
    "    print(\"saved\", path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial run and output production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resp.status_code == 200\n",
      "reddit error worldnews Blocked by robots.txt for https://old.reddit.com/r/worldnews/\n",
      "reddit error news Blocked by robots.txt for https://old.reddit.com/r/news/\n",
      "resp.status_code == 200\n",
      "news error reuters_world Fetch failed after retries for https://www.reuters.com/world/\n",
      "saved storage/raw/crawl_20250911_004252.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_type</th>\n",
       "      <th>source_name</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>url</th>\n",
       "      <th>canonical_url</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>published_at</th>\n",
       "      <th>score</th>\n",
       "      <th>comments</th>\n",
       "      <th>language</th>\n",
       "      <th>token_count</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>label_scores</th>\n",
       "      <th>fetched_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [source_type, source_name, subreddit, url, canonical_url, title, text, author, published_at, score, comments, language, token_count, predicted_label, label_scores, fetched_at]\n",
       "Index: []"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "with open(\"configs/crawl_config.yaml\", \"r\", encoding=\"utf-8\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "all_records = []\n",
    "\n",
    "# Reddit\n",
    "for sub in cfg[\"sources\"][\"reddit\"][\"subreddits\"]:\n",
    "    try:\n",
    "        recs = crawl_reddit_subreddit(sub, cfg)\n",
    "        all_records.extend(recs)\n",
    "        print(f\"reddit {sub} records:\", len(recs))\n",
    "    except Exception as e:\n",
    "        print(\"reddit error\", sub, e)\n",
    "\n",
    "# News\n",
    "for site in cfg[\"sources\"][\"news_sites\"]:\n",
    "    try:\n",
    "        recs = crawl_news_site(site, cfg)\n",
    "        all_records.extend(recs)\n",
    "        print(f\"news {site['name']} records:\", len(recs))\n",
    "    except Exception as e:\n",
    "        print(\"news error\", site[\"name\"], e)\n",
    "\n",
    "df = to_dataframe(all_records)\n",
    "ts = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
    "out_path = f\"storage/raw/crawl_{ts}.csv\"\n",
    "save_csv(df, out_path)\n",
    "df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
