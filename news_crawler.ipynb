{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> imports <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, threading, random, re, urllib.parse as urlparse, yaml, requests, pandas as pd\n",
    "from collections import defaultdict\n",
    "from urllib.robotparser import RobotFileParser\n",
    "from bs4 import BeautifulSoup\n",
    "from hashlib import sha1\n",
    "import tldextract\n",
    "from datetime import datetime\n",
    "from readability import Document\n",
    "import feedparser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 1.\tCrawler / Scraper <h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining basic settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs(\"configs\", exist_ok=True)\n",
    "# os.makedirs(\"storage/raw\", exist_ok=True)\n",
    "# os.makedirs(\"storage/clean\", exist_ok=True)\n",
    "# os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "# CONFIG = {\n",
    "#     \"user_agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36\",\n",
    "#     \"default_timeout_sec\": 15,\n",
    "#     \"per_host_min_delay_sec\": 2,\n",
    "#     \"max_retries\": 3,\n",
    "#     \"sources\": {\n",
    "#         \"reddit\": {\n",
    "#             \"type\": \"reddit_html\",\n",
    "#             \"subreddits\": [\"worldnews\", \"news\"],\n",
    "#             \"limit_per_sub\": 0\n",
    "#         },\n",
    "#         \"news_sites\": [\n",
    "#             {\n",
    "#                 \"name\": \"reuters_world\",\n",
    "#                 \"start_urls\": [\"https://www.reuters.com/world/\"],\n",
    "#                 \"article_selector_hint\": \"a\"\n",
    "#             }\n",
    "#         ]\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# with open(\"configs/crawl_config.yaml\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     yaml.safe_dump(CONFIG, f, allow_unicode=True)\n",
    "# print(\"configs/crawl_config.yaml saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configs/crawl_config.yaml saved\n"
     ]
    }
   ],
   "source": [
    "import os, yaml\n",
    "\n",
    "os.makedirs(\"configs\", exist_ok=True)\n",
    "os.makedirs(\"storage/raw\", exist_ok=True)\n",
    "os.makedirs(\"storage/clean\", exist_ok=True)\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "CONFIG = {\n",
    "    \"user_agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36\",\n",
    "    \"default_timeout_sec\": 15,\n",
    "    \"per_host_min_delay_sec\": 2,\n",
    "    \"max_retries\": 3,\n",
    "    \"sources\": {\n",
    "        \"reddit\": {\n",
    "            \"type\": \"reddit_html\",\n",
    "            \"subreddits\": [\n",
    "                \"worldnews\",\n",
    "                \"news\",\n",
    "                \"politics\",\n",
    "                \"technology\",\n",
    "                \"science\",\n",
    "                \"environment\",\n",
    "                \"economics\"\n",
    "            ],\n",
    "            \"limit_per_sub\": 50\n",
    "        },\n",
    "        \"npr_news\": {\n",
    "            \"type\": \"news_html\",\n",
    "            \"start_urls\": [\"https://www.npr.org/sections/world/\"],\n",
    "            \"article_selector_hint\": \"a\",\n",
    "            \"include_patterns\": [\"/story/\", \"/202\", \"/sections/world/\"],\n",
    "            \"limit_per_section\": 50\n",
    "        },\n",
    "        \"news_sites\": [\n",
    "            # {\n",
    "            #     \"name\": \"ap_world\",\n",
    "            #     \"start_urls\": [\"https://apnews.com/hub/world-news\"],\n",
    "            #     \"article_selector_hint\": \"a\"\n",
    "            # }\n",
    "            {\n",
    "                \"name\": \"ap_world\",\n",
    "                \"start_urls\": [\"https://apnews.com/hub/world-news\"],\n",
    "                \"article_selector_hint\": \"a\",\n",
    "                \"include_patterns\": [\"/article/\"]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"guardian_world\",\n",
    "                \"start_urls\": [\"https://www.theguardian.com/world\"],\n",
    "                \"article_selector_hint\": \"a\",\n",
    "                \"include_patterns\": [\"/world/\"]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"bbc_world\",\n",
    "                \"start_urls\": [\"https://www.bbc.com/news/world\"],\n",
    "                \"article_selector_hint\": \"a\",\n",
    "                \"include_patterns\": [\"/news/\"]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(\"configs/crawl_config.yaml\", \"w\", encoding=\"utf-8\") as f:\n",
    "    yaml.safe_dump(CONFIG, f, allow_unicode=True)\n",
    "print(\"configs/crawl_config.yaml saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the robots.txt and control the rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "_session = requests.Session()\n",
    "# _session.headers.update({\"Accept-Language\": \"en;q=0.9\"})\n",
    "_session.headers.update({\n",
    "    \"Accept-Language\": \"en;q=0.9\",\n",
    "    \"User-Agent\": CONFIG[\"user_agent\"],\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "    \"Connection\": \"keep-alive\"\n",
    "})\n",
    "\n",
    "# ==== robots.txt handling and rate limit ====\n",
    "_robot_cache = {}\n",
    "_host_next_time = defaultdict(float)\n",
    "_lock = threading.Lock()\n",
    "\n",
    "def get_robots_parser(base_url, ua):\n",
    "    parsed = urlparse.urlparse(base_url)\n",
    "    robots_url = f\"{parsed.scheme}://{parsed.netloc}/robots.txt\"\n",
    "    if robots_url in _robot_cache:\n",
    "        return _robot_cache[robots_url]\n",
    "    rp = RobotFileParser()\n",
    "    try:\n",
    "        resp = _session.get(robots_url, timeout=8)\n",
    "        if resp.status_code == 200:\n",
    "            rp.parse(resp.text.splitlines())\n",
    "        else:\n",
    "            rp.parse([])\n",
    "    except Exception:\n",
    "        rp.parse([])\n",
    "    _robot_cache[robots_url] = rp\n",
    "    return rp\n",
    "\n",
    "def host_key(url):\n",
    "    p = urlparse.urlparse(url)\n",
    "    ext = tldextract.extract(p.netloc)\n",
    "    return \".\".join([x for x in [ext.domain, ext.suffix] if x])\n",
    "\n",
    "def rate_limit(url, min_delay_sec):\n",
    "    hk = host_key(url)\n",
    "    with _lock:\n",
    "        now = time.time()\n",
    "        nt = _host_next_time[hk]\n",
    "        wait = nt - now\n",
    "        if wait > 0:\n",
    "            time.sleep(wait)\n",
    "        _host_next_time[hk] = time.time() + min_delay_sec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch function (with Retry & Exponential Backoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch(url, user_agent, timeout=15, max_retries=3, min_delay_sec=2):\n",
    "    rp = get_robots_parser(url, user_agent)\n",
    "    if not rp.can_fetch(user_agent, url):\n",
    "        raise PermissionError(f\"Blocked by robots.txt for {url}\")\n",
    "\n",
    "    rate_limit(url, min_delay_sec)\n",
    "\n",
    "    headers = {\"User-Agent\": user_agent}\n",
    "    attempt = 0\n",
    "    backoff = 1.6\n",
    "\n",
    "    while attempt <= max_retries:\n",
    "        try:\n",
    "            resp = _session.get(url, headers=headers, timeout=timeout, allow_redirects=True)\n",
    "\n",
    "            if 200 <= resp.status_code < 300:\n",
    "                return resp\n",
    "\n",
    "            # محدودیت نرخ و خطای موقت\n",
    "            if resp.status_code in (429, 502, 503, 504):\n",
    "                sleep_sec = (backoff ** attempt) + random.uniform(0, 0.5)\n",
    "                time.sleep(sleep_sec)\n",
    "\n",
    "            # عدم دسترسی یا محدودیت قانونی\n",
    "            elif resp.status_code in (401, 403, 451):\n",
    "                break\n",
    "\n",
    "            else:\n",
    "                time.sleep(0.6)\n",
    "\n",
    "        except requests.RequestException:\n",
    "            time.sleep((backoff ** attempt) + 0.4)\n",
    "\n",
    "        attempt += 1\n",
    "\n",
    "    raise TimeoutError(f\"Fetch failed after retries for {url}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML Crawler for Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "UPVOTE_RE = re.compile(r\"(\\d+(?:\\.\\d+)?)([kK])?\\s*upvote\")\n",
    "\n",
    "def to_int_k(v):\n",
    "    if v is None:\n",
    "        return None\n",
    "    s = str(v).strip().lower()\n",
    "    if s.endswith(\"k\"):\n",
    "        try:\n",
    "            return int(float(s[:-1]) * 1000)\n",
    "        except:\n",
    "            return None\n",
    "    try:\n",
    "        return int(s)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parse_reddit_listing(html, base_url):\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    items = []\n",
    "    for post in soup.select(\"[data-testid='post-container'], div[data-test-id='post-content']\"):\n",
    "        title_el = post.select_one(\"h3\") or post.select_one(\"a[data-click-id='body'] h3\")\n",
    "        if not title_el:\n",
    "            continue\n",
    "        title = title_el.get_text(strip=True)\n",
    "        # link_el = post.select_one(\"a[data-click-id='body']\") or post.find(\"a\", href=True)\n",
    "        link_el = post.select_one(\"a[data-click-id='body'][href]\") or post.select_one(\"a[href^='/r/']\")\n",
    "\n",
    "        url = urlparse.urljoin(base_url, link_el[\"href\"]) if link_el and link_el.get(\"href\") else None\n",
    "        if not url:\n",
    "            continue\n",
    "        \n",
    "        author_el = post.select_one(\"a[data-click-id='user']\") or post.select_one(\"a[href^='/user/']\")\n",
    "        author = author_el.get_text(strip=True) if author_el else None\n",
    "        time_el = post.select_one(\"a[data-click-id='timestamp'] time\") or post.find(\"time\")\n",
    "        published_at = time_el.get(\"datetime\") if time_el and time_el.has_attr(\"datetime\") else None\n",
    "        comments_el = post.select_one(\"a[data-click-id='comments']\") or post.find(\"a\", string=re.compile(\"comment\", re.I))\n",
    "        comments = None\n",
    "        if comments_el:\n",
    "            m = re.search(r\"(\\d+(?:\\.\\d+)?[kK]?)\", comments_el.get_text(\" \", strip=True))\n",
    "            if m:\n",
    "                comments = to_int_k(m.group(1))\n",
    "        score = None\n",
    "        aria_up = post.find(attrs={\"aria-label\": re.compile(\"upvote\", re.I)})\n",
    "        if aria_up:\n",
    "            m = UPVOTE_RE.search(aria_up.get(\"aria-label\", \"\"))\n",
    "            if m:\n",
    "                val = float(m.group(1))\n",
    "                if m.group(2):\n",
    "                    val *= 1000\n",
    "                score = int(val)\n",
    "        items.append({\n",
    "            \"source_type\": \"reddit\",\n",
    "            \"source_name\": base_url,\n",
    "            \"subreddit\": base_url.rstrip(\"/\").split(\"/\")[-1],\n",
    "            \"url\": url,\n",
    "            \"canonical_url\": url,\n",
    "            \"title\": title,\n",
    "            \"text\": None,\n",
    "            \"author\": author,\n",
    "            \"published_at\": published_at,\n",
    "            \"score\": score,\n",
    "            \"comments\": comments,\n",
    "            \"fetched_at\": datetime.utcnow().isoformat(timespec=\"seconds\")\n",
    "        })\n",
    "    return items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "STRIP_PARAMS = {\"utm_source\",\"utm_medium\",\"utm_campaign\",\"utm_term\",\"utm_content\",\"ref\",\"utm_name\",\"gclid\",\"fbclid\"}\n",
    "\n",
    "def normalize_url(u: str) -> str | None:\n",
    "    if not u:\n",
    "        return None\n",
    "    p = urlparse.urlparse(u)\n",
    "    if p.scheme not in (\"http\",\"https\"):\n",
    "        return None\n",
    "    q = [(k,v) for k,v in urlparse.parse_qsl(p.query, keep_blank_values=True) if k not in STRIP_PARAMS]\n",
    "    new_q = urlparse.urlencode(q)\n",
    "    return urlparse.urlunparse((p.scheme, p.netloc, p.path, \"\", new_q, \"\"))\n",
    "\n",
    "def dedupe_records(records):\n",
    "    seen = set()\n",
    "    unique = []\n",
    "    for r in records:\n",
    "        key = normalize_url(r.get(\"canonical_url\") or r.get(\"url\")) or r.get(\"url\")\n",
    "        if not key:\n",
    "            continue\n",
    "        h = sha1(key.encode(\"utf-8\")).hexdigest()\n",
    "        if h in seen:\n",
    "            continue\n",
    "        seen.add(h)\n",
    "        r[\"canonical_url\"] = key\n",
    "        unique.append(r)\n",
    "    return unique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reddit_rss_url(sub: str) -> str:\n",
    "    # RSS پایدار روی www.reddit.com بهتر جواب می‌دهد\n",
    "    return f\"https://www.reddit.com/r/{sub}/.rss\"\n",
    "\n",
    "def crawl_reddit_via_rss(sub, cfg, limit=None):\n",
    "    rss = reddit_rss_url(sub)\n",
    "    ua = cfg[\"user_agent\"]\n",
    "\n",
    "    # robots برای خود مسیر RSS چک شود\n",
    "    rp = get_robots_parser(rss, ua)\n",
    "    if not rp.can_fetch(ua, rss):\n",
    "        print(f\"robots disallows RSS for {rss}\")\n",
    "        return []\n",
    "\n",
    "    # با Session خودمان fetch کنیم تا UA درست ارسال شود\n",
    "    resp = fetch(rss, ua,\n",
    "                 timeout=cfg[\"default_timeout_sec\"],\n",
    "                 max_retries=cfg[\"max_retries\"],\n",
    "                 min_delay_sec=cfg[\"per_host_min_delay_sec\"])\n",
    "\n",
    "    print(\"rss status:\", resp.status_code, \"bytes:\", len(resp.text))\n",
    "\n",
    "    d = feedparser.parse(resp.text)\n",
    "    print(\"rss entries:\", len(d.get(\"entries\", [])))\n",
    "    items = []\n",
    "    now = datetime.utcnow().isoformat(timespec=\"seconds\")\n",
    "    for e in d.get(\"entries\", [])[: (limit or 50)]:\n",
    "        link = e.get(\"link\")\n",
    "        items.append({\n",
    "            \"source_type\": \"reddit\",\n",
    "            \"source_name\": rss,\n",
    "            \"subreddit\": sub,\n",
    "            \"url\": link,\n",
    "            \"canonical_url\": normalize_url(link) if link else link,\n",
    "            \"title\": e.get(\"title\"),\n",
    "            \"text\": None,\n",
    "            \"author\": e.get(\"author\") if \"author\" in e else None,\n",
    "            \"published_at\": e.get(\"published\") if \"published\" in e else None,\n",
    "            \"score\": None,\n",
    "            \"comments\": None,\n",
    "            \"fetched_at\": now\n",
    "        })\n",
    "    return dedupe_records(items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_reddit_subreddit(sub, cfg):\n",
    "    ua = cfg[\"user_agent\"]\n",
    "    limit = cfg[\"sources\"][\"reddit\"][\"limit_per_sub\"]\n",
    "    html_url = f\"https://old.reddit.com/r/{sub}/\"\n",
    "    rp = get_robots_parser(html_url, ua)\n",
    "    if not rp.can_fetch(ua, html_url):\n",
    "        print(f\"robots disallows HTML for {html_url}, switching to RSS\")\n",
    "        return crawl_reddit_via_rss(sub, cfg, limit)\n",
    "    # اگر HTML مجاز بود همان مسیر قبلی\n",
    "    resp = fetch(html_url, ua,\n",
    "                 timeout=cfg[\"default_timeout_sec\"],\n",
    "                 max_retries=cfg[\"max_retries\"],\n",
    "                 min_delay_sec=cfg[\"per_host_min_delay_sec\"])\n",
    "    items = parse_reddit_listing(resp.text, html_url)\n",
    "    items = dedupe_records(items)\n",
    "    return items[:limit] if limit else items\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crawl news site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Article text extraction ====\n",
    "def extract_article_text(html):\n",
    "    try:\n",
    "        doc = Document(html)\n",
    "        content_html = doc.summary()\n",
    "        soup = BeautifulSoup(content_html, \"lxml\")\n",
    "        text = soup.get_text(\"\\n\", strip=True)\n",
    "        if len(text) < 200:\n",
    "            soup_full = BeautifulSoup(html, \"lxml\")\n",
    "            paras = [p.get_text(\" \", strip=True) for p in soup_full.select(\"p\")]\n",
    "            text = \"\\n\".join(paras[:60])\n",
    "        return text\n",
    "    except Exception:\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        paras = [p.get_text(\" \", strip=True) for p in soup.select(\"p\")]\n",
    "        return \"\\n\".join(paras[:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Link discovery for news sites ====\n",
    "def parse_listing_find_links(html, base_url, selector_hint=\"a\", include_patterns=None):\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    base = urlparse.urlparse(base_url).netloc\n",
    "    links = []\n",
    "    for a in soup.select(selector_hint):\n",
    "        href = a.get(\"href\")\n",
    "        if not href:\n",
    "            continue\n",
    "        full = urlparse.urljoin(base_url, href)\n",
    "        norm = normalize_url(full)\n",
    "        if not norm:\n",
    "            continue\n",
    "        if urlparse.urlparse(norm).netloc != base:\n",
    "            continue\n",
    "        if include_patterns and not any(p in norm for p in include_patterns):\n",
    "            continue\n",
    "        links.append(norm.rstrip(\"#\"))\n",
    "    return list(dict.fromkeys(links))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Consent page detection ====\n",
    "def looks_like_consent_page(html: str) -> bool:\n",
    "    s = html.lower()\n",
    "    return (\"consent\" in s) or (\"gdpr\" in s) or (\"privacy preferences\" in s) or (\"iab\" in s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# article meta remains the same\n",
    "def extract_article_meta(html: str, url: str):\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    title = None\n",
    "    for sel in [\"meta[property='og:title']\", \"meta[name='twitter:title']\", \"title\"]:\n",
    "        el = soup.select_one(sel)\n",
    "        if el:\n",
    "            title = el.get(\"content\") if el.has_attr(\"content\") else el.get_text(strip=True)\n",
    "        if title:\n",
    "            break\n",
    "    author = None\n",
    "    for sel in [\"meta[name='author']\", \"meta[property='article:author']\", \"a[rel='author']\"]:\n",
    "        el = soup.select_one(sel)\n",
    "        if el:\n",
    "            author = el.get(\"content\") if el.has_attr(\"content\") else el.get_text(strip=True)\n",
    "        if author:\n",
    "            break\n",
    "    published_at = None\n",
    "    for sel in [\"meta[property='article:published_time']\", \"meta[name='pubdate']\", \"time[datetime]\"]:\n",
    "        el = soup.select_one(sel)\n",
    "        if el:\n",
    "            published_at = el.get(\"content\") if el.has_attr(\"content\") else el.get(\"datetime\")\n",
    "        if published_at:\n",
    "            break\n",
    "    can = soup.find(\"link\", rel=lambda x: x and \"canonical\" in x.lower())\n",
    "    canonical = urlparse.urljoin(url, can[\"href\"]) if can and can.get(\"href\") else url\n",
    "    canonical = normalize_url(canonical)\n",
    "    return title, author, published_at, canonical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawl news site with include_patterns support\n",
    "def crawl_news_site(entry, cfg, max_article_per_listing=30, min_text_len=300):\n",
    "    ua = cfg[\"user_agent\"]\n",
    "    start_urls = entry[\"start_urls\"]\n",
    "    selector_hint = entry.get(\"article_selector_hint\", \"a\")\n",
    "    include_patterns = entry.get(\"include_patterns\")  # از کانفیگ\n",
    "    records = []\n",
    "    seen = set()\n",
    "\n",
    "    for su in start_urls:\n",
    "        resp = fetch(su, ua,\n",
    "                     timeout=cfg[\"default_timeout_sec\"],\n",
    "                     max_retries=cfg[\"max_retries\"],\n",
    "                     min_delay_sec=cfg[\"per_host_min_delay_sec\"])\n",
    "        links = parse_listing_find_links(resp.text, su, selector_hint, include_patterns)\n",
    "\n",
    "        for lk in links[:max_article_per_listing]:\n",
    "            try:\n",
    "                art = fetch(lk, ua,\n",
    "                            timeout=cfg[\"default_timeout_sec\"],\n",
    "                            max_retries=cfg[\"max_retries\"],\n",
    "                            min_delay_sec=cfg[\"per_host_min_delay_sec\"])\n",
    "\n",
    "                text = extract_article_text(art.text) or \"\"\n",
    "                if len(text) < min_text_len:\n",
    "                    # اگر کوتاه است و نشانه consent دارد، رد کن\n",
    "                    # if looks_like_consent_page(art.text):\n",
    "                        # print(\"skip consent page:\", lk)\n",
    "                    continue\n",
    "\n",
    "                title, author, published_at, canonical = extract_article_meta(art.text, lk)\n",
    "                canonical = canonical or normalize_url(lk)\n",
    "                if not canonical:\n",
    "                    continue\n",
    "\n",
    "                h = sha1(canonical.encode(\"utf-8\")).hexdigest()\n",
    "                if h in seen:\n",
    "                    continue\n",
    "                seen.add(h)\n",
    "\n",
    "                records.append({\n",
    "                    \"source_type\": \"news\",\n",
    "                    \"source_name\": entry[\"name\"],\n",
    "                    \"subreddit\": None,\n",
    "                    \"url\": lk,\n",
    "                    \"canonical_url\": canonical,\n",
    "                    \"title\": title,\n",
    "                    \"text\": text,\n",
    "                    \"author\": author,\n",
    "                    \"published_at\": published_at,\n",
    "                    \"score\": None,\n",
    "                    \"comments\": None,\n",
    "                    \"fetched_at\": datetime.utcnow().isoformat(timespec=\"seconds\")\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(\"error on\", lk, e)\n",
    "    return records\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSV Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== CSV Schema and helpers ====\n",
    "CSV_SCHEMA = [\n",
    "    \"source_type\", \"source_name\", \"subreddit\", \"url\", \"canonical_url\",\n",
    "    \"title\", \"text\", \"author\", \"published_at\", \"score\", \"comments\",\n",
    "    \"language\", \"token_count\", \"predicted_label\", \"label_scores\", \"fetched_at\"\n",
    "]\n",
    "\n",
    "def to_dataframe(records):\n",
    "    df = pd.DataFrame(records)\n",
    "    for col in CSV_SCHEMA:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "    return df[CSV_SCHEMA]\n",
    "\n",
    "def save_csv(df, path):\n",
    "    df.to_csv(path, index=False, encoding=\"utf-8\")\n",
    "    print(\"saved\", path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "robots disallows HTML for https://old.reddit.com/r/worldnews/, switching to RSS\n",
      "robots disallows RSS for https://www.reddit.com/r/worldnews/.rss\n",
      "reddit worldnews records: 0\n",
      "robots disallows HTML for https://old.reddit.com/r/news/, switching to RSS\n",
      "robots disallows RSS for https://www.reddit.com/r/news/.rss\n",
      "reddit news records: 0\n",
      "robots disallows HTML for https://old.reddit.com/r/politics/, switching to RSS\n",
      "robots disallows RSS for https://www.reddit.com/r/politics/.rss\n",
      "reddit politics records: 0\n",
      "robots disallows HTML for https://old.reddit.com/r/technology/, switching to RSS\n",
      "robots disallows RSS for https://www.reddit.com/r/technology/.rss\n",
      "reddit technology records: 0\n",
      "robots disallows HTML for https://old.reddit.com/r/science/, switching to RSS\n",
      "robots disallows RSS for https://www.reddit.com/r/science/.rss\n",
      "reddit science records: 0\n",
      "robots disallows HTML for https://old.reddit.com/r/environment/, switching to RSS\n",
      "robots disallows RSS for https://www.reddit.com/r/environment/.rss\n",
      "reddit environment records: 0\n",
      "robots disallows HTML for https://old.reddit.com/r/economics/, switching to RSS\n",
      "robots disallows RSS for https://www.reddit.com/r/economics/.rss\n",
      "reddit economics records: 0\n",
      "news ap_world records: 30\n",
      "skip consent page: https://www.theguardian.com/world/ukraine\n",
      "skip consent page: https://www.theguardian.com/world/europe-news\n",
      "skip consent page: https://www.theguardian.com/world/americas\n",
      "skip consent page: https://www.theguardian.com/world/asia\n",
      "skip consent page: https://www.theguardian.com/world/africa\n",
      "skip consent page: https://www.theguardian.com/world/asia-pacific\n",
      "skip consent page: https://www.theguardian.com/world/south-and-central-asia\n",
      "news guardian_world records: 23\n",
      "skip consent page: https://www.bbc.com/news/world\n",
      "skip consent page: https://www.bbc.com/news/us-canada\n",
      "skip consent page: https://www.bbc.com/news/politics\n",
      "skip consent page: https://www.bbc.com/news/world/europe\n",
      "skip consent page: https://www.bbc.com/news/world/latin_america\n",
      "skip consent page: https://www.bbc.com/news/bbcindepth\n",
      "skip consent page: https://www.bbc.com/news/bbcverify\n",
      "news bbc_world records: 23\n",
      "saved storage/raw/crawl_20250911_155214.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_type</th>\n",
       "      <th>source_name</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>url</th>\n",
       "      <th>canonical_url</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>published_at</th>\n",
       "      <th>score</th>\n",
       "      <th>comments</th>\n",
       "      <th>language</th>\n",
       "      <th>token_count</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>label_scores</th>\n",
       "      <th>fetched_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>news</td>\n",
       "      <td>ap_world</td>\n",
       "      <td>None</td>\n",
       "      <td>https://apnews.com/article/nato-poland-russia-...</td>\n",
       "      <td>https://apnews.com/article/nato-poland-russia-...</td>\n",
       "      <td>What NATO's Article 4 talks mean after Russian...</td>\n",
       "      <td>BRUSSELS (AP) —\\nNATO\\nallies swiftly held tal...</td>\n",
       "      <td>https://apnews.com/author/the-associated-press</td>\n",
       "      <td>2025-09-10T12:59:34</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-09-11T15:49:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>news</td>\n",
       "      <td>ap_world</td>\n",
       "      <td>None</td>\n",
       "      <td>https://apnews.com/article/uk-mandelson-epstei...</td>\n",
       "      <td>https://apnews.com/article/uk-mandelson-epstei...</td>\n",
       "      <td>UK fires ambassador to the US Peter Mandelson ...</td>\n",
       "      <td>LONDON (AP) — U.K. Prime Minister Keir Starmer...</td>\n",
       "      <td>https://apnews.com/author/pan-pylas</td>\n",
       "      <td>2025-09-11T09:55:51</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-09-11T15:49:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>news</td>\n",
       "      <td>ap_world</td>\n",
       "      <td>None</td>\n",
       "      <td>https://apnews.com/article/prince-harry-king-c...</td>\n",
       "      <td>https://apnews.com/article/prince-harry-king-c...</td>\n",
       "      <td>Prince Harry has tea with his father, King Cha...</td>\n",
       "      <td>LONDON (AP) — Britain’s\\nPrince Harry\\njoined ...</td>\n",
       "      <td>https://apnews.com/author/danica-kirka</td>\n",
       "      <td>2025-09-10T16:52:03</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-09-11T15:49:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>news</td>\n",
       "      <td>ap_world</td>\n",
       "      <td>None</td>\n",
       "      <td>https://apnews.com/article/charlie-kirk-conser...</td>\n",
       "      <td>https://apnews.com/article/charlie-kirk-conser...</td>\n",
       "      <td>Conservative activist Charlie Kirk assassinate...</td>\n",
       "      <td>OREM, Utah (AP) —\\nCharlie Kirk, a conservativ...</td>\n",
       "      <td>https://apnews.com/author/hannah-schoenbaum</td>\n",
       "      <td>2025-09-10T19:01:06</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-09-11T15:49:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>news</td>\n",
       "      <td>ap_world</td>\n",
       "      <td>None</td>\n",
       "      <td>https://apnews.com/article/charlie-kirk-video-...</td>\n",
       "      <td>https://apnews.com/article/charlie-kirk-video-...</td>\n",
       "      <td>Graphic video of Kirk shooting was everywhere ...</td>\n",
       "      <td>They were careful with the explicit imagery — ...</td>\n",
       "      <td>https://apnews.com/author/david-bauder</td>\n",
       "      <td>2025-09-11T00:57:39</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-09-11T15:49:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>news</td>\n",
       "      <td>ap_world</td>\n",
       "      <td>None</td>\n",
       "      <td>https://apnews.com/article/charlie-kirk-shooti...</td>\n",
       "      <td>https://apnews.com/article/charlie-kirk-shooti...</td>\n",
       "      <td>Charlie Kirk, who helped build support for Tru...</td>\n",
       "      <td>Charlie Kirk, who rose from\\na teenage conserv...</td>\n",
       "      <td>https://apnews.com/author/nicholas-riccardi</td>\n",
       "      <td>2025-09-10T19:49:41</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-09-11T15:49:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>news</td>\n",
       "      <td>ap_world</td>\n",
       "      <td>None</td>\n",
       "      <td>https://apnews.com/article/charlie-kirk-assass...</td>\n",
       "      <td>https://apnews.com/article/charlie-kirk-assass...</td>\n",
       "      <td>A college campus, a fiery speaker — and then a...</td>\n",
       "      <td>OREM, Utah (AP) — Just weeks into the fall sem...</td>\n",
       "      <td>https://apnews.com/author/gene-johnson</td>\n",
       "      <td>2025-09-11T03:02:07</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-09-11T15:49:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>news</td>\n",
       "      <td>ap_world</td>\n",
       "      <td>None</td>\n",
       "      <td>https://apnews.com/article/jeffrey-epstein-fil...</td>\n",
       "      <td>https://apnews.com/article/jeffrey-epstein-fil...</td>\n",
       "      <td>Senate Republicans defeat Democrats' effort to...</td>\n",
       "      <td>WASHINGTON (AP) — In a close vote, Senate Repu...</td>\n",
       "      <td>https://apnews.com/author/stephen-groves</td>\n",
       "      <td>2025-09-10T22:31:21</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-09-11T15:49:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>news</td>\n",
       "      <td>ap_world</td>\n",
       "      <td>None</td>\n",
       "      <td>https://apnews.com/article/cubs-anthony-rizzo-...</td>\n",
       "      <td>https://apnews.com/article/cubs-anthony-rizzo-...</td>\n",
       "      <td>Anthony Rizzo will retire as a Chicago Cub and...</td>\n",
       "      <td>CHICAGO (AP) — Anthony Rizzo will officially r...</td>\n",
       "      <td>https://apnews.com/author/rick-farlow</td>\n",
       "      <td>2025-09-10T16:13:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-09-11T15:49:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>news</td>\n",
       "      <td>ap_world</td>\n",
       "      <td>None</td>\n",
       "      <td>https://apnews.com/article/gambling-ncaa-fresn...</td>\n",
       "      <td>https://apnews.com/article/gambling-ncaa-fresn...</td>\n",
       "      <td>NCAA bans 3 college basketball players for bet...</td>\n",
       "      <td>The NCAA banned three men’s college basketball...</td>\n",
       "      <td>https://apnews.com/author/cliff-brunt</td>\n",
       "      <td>2025-09-10T16:37:41</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-09-11T15:49:30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source_type source_name subreddit  \\\n",
       "0        news    ap_world      None   \n",
       "1        news    ap_world      None   \n",
       "2        news    ap_world      None   \n",
       "3        news    ap_world      None   \n",
       "4        news    ap_world      None   \n",
       "5        news    ap_world      None   \n",
       "6        news    ap_world      None   \n",
       "7        news    ap_world      None   \n",
       "8        news    ap_world      None   \n",
       "9        news    ap_world      None   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://apnews.com/article/nato-poland-russia-...   \n",
       "1  https://apnews.com/article/uk-mandelson-epstei...   \n",
       "2  https://apnews.com/article/prince-harry-king-c...   \n",
       "3  https://apnews.com/article/charlie-kirk-conser...   \n",
       "4  https://apnews.com/article/charlie-kirk-video-...   \n",
       "5  https://apnews.com/article/charlie-kirk-shooti...   \n",
       "6  https://apnews.com/article/charlie-kirk-assass...   \n",
       "7  https://apnews.com/article/jeffrey-epstein-fil...   \n",
       "8  https://apnews.com/article/cubs-anthony-rizzo-...   \n",
       "9  https://apnews.com/article/gambling-ncaa-fresn...   \n",
       "\n",
       "                                       canonical_url  \\\n",
       "0  https://apnews.com/article/nato-poland-russia-...   \n",
       "1  https://apnews.com/article/uk-mandelson-epstei...   \n",
       "2  https://apnews.com/article/prince-harry-king-c...   \n",
       "3  https://apnews.com/article/charlie-kirk-conser...   \n",
       "4  https://apnews.com/article/charlie-kirk-video-...   \n",
       "5  https://apnews.com/article/charlie-kirk-shooti...   \n",
       "6  https://apnews.com/article/charlie-kirk-assass...   \n",
       "7  https://apnews.com/article/jeffrey-epstein-fil...   \n",
       "8  https://apnews.com/article/cubs-anthony-rizzo-...   \n",
       "9  https://apnews.com/article/gambling-ncaa-fresn...   \n",
       "\n",
       "                                               title  \\\n",
       "0  What NATO's Article 4 talks mean after Russian...   \n",
       "1  UK fires ambassador to the US Peter Mandelson ...   \n",
       "2  Prince Harry has tea with his father, King Cha...   \n",
       "3  Conservative activist Charlie Kirk assassinate...   \n",
       "4  Graphic video of Kirk shooting was everywhere ...   \n",
       "5  Charlie Kirk, who helped build support for Tru...   \n",
       "6  A college campus, a fiery speaker — and then a...   \n",
       "7  Senate Republicans defeat Democrats' effort to...   \n",
       "8  Anthony Rizzo will retire as a Chicago Cub and...   \n",
       "9  NCAA bans 3 college basketball players for bet...   \n",
       "\n",
       "                                                text  \\\n",
       "0  BRUSSELS (AP) —\\nNATO\\nallies swiftly held tal...   \n",
       "1  LONDON (AP) — U.K. Prime Minister Keir Starmer...   \n",
       "2  LONDON (AP) — Britain’s\\nPrince Harry\\njoined ...   \n",
       "3  OREM, Utah (AP) —\\nCharlie Kirk, a conservativ...   \n",
       "4  They were careful with the explicit imagery — ...   \n",
       "5  Charlie Kirk, who rose from\\na teenage conserv...   \n",
       "6  OREM, Utah (AP) — Just weeks into the fall sem...   \n",
       "7  WASHINGTON (AP) — In a close vote, Senate Repu...   \n",
       "8  CHICAGO (AP) — Anthony Rizzo will officially r...   \n",
       "9  The NCAA banned three men’s college basketball...   \n",
       "\n",
       "                                           author         published_at score  \\\n",
       "0  https://apnews.com/author/the-associated-press  2025-09-10T12:59:34  None   \n",
       "1             https://apnews.com/author/pan-pylas  2025-09-11T09:55:51  None   \n",
       "2          https://apnews.com/author/danica-kirka  2025-09-10T16:52:03  None   \n",
       "3     https://apnews.com/author/hannah-schoenbaum  2025-09-10T19:01:06  None   \n",
       "4          https://apnews.com/author/david-bauder  2025-09-11T00:57:39  None   \n",
       "5     https://apnews.com/author/nicholas-riccardi  2025-09-10T19:49:41  None   \n",
       "6          https://apnews.com/author/gene-johnson  2025-09-11T03:02:07  None   \n",
       "7        https://apnews.com/author/stephen-groves  2025-09-10T22:31:21  None   \n",
       "8           https://apnews.com/author/rick-farlow  2025-09-10T16:13:00  None   \n",
       "9           https://apnews.com/author/cliff-brunt  2025-09-10T16:37:41  None   \n",
       "\n",
       "  comments language token_count predicted_label label_scores  \\\n",
       "0     None     None        None            None         None   \n",
       "1     None     None        None            None         None   \n",
       "2     None     None        None            None         None   \n",
       "3     None     None        None            None         None   \n",
       "4     None     None        None            None         None   \n",
       "5     None     None        None            None         None   \n",
       "6     None     None        None            None         None   \n",
       "7     None     None        None            None         None   \n",
       "8     None     None        None            None         None   \n",
       "9     None     None        None            None         None   \n",
       "\n",
       "            fetched_at  \n",
       "0  2025-09-11T15:49:12  \n",
       "1  2025-09-11T15:49:14  \n",
       "2  2025-09-11T15:49:16  \n",
       "3  2025-09-11T15:49:18  \n",
       "4  2025-09-11T15:49:20  \n",
       "5  2025-09-11T15:49:22  \n",
       "6  2025-09-11T15:49:24  \n",
       "7  2025-09-11T15:49:26  \n",
       "8  2025-09-11T15:49:28  \n",
       "9  2025-09-11T15:49:30  "
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==== Run the crawlers ====\n",
    "with open(\"configs/crawl_config.yaml\", \"r\", encoding=\"utf-8\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "all_records = []\n",
    "\n",
    "# Reddit\n",
    "for sub in cfg[\"sources\"][\"reddit\"][\"subreddits\"]:\n",
    "    try:\n",
    "        recs = crawl_reddit_subreddit(sub, cfg)\n",
    "        all_records.extend(recs)\n",
    "        print(f\"reddit {sub} records:\", len(recs))\n",
    "    except Exception as e:\n",
    "        print(\"reddit error\", sub, e)\n",
    "\n",
    "# News\n",
    "for site in cfg[\"sources\"][\"news_sites\"]:\n",
    "    try:\n",
    "        recs = crawl_news_site(site, cfg)\n",
    "        all_records.extend(recs)\n",
    "        print(f\"news {site['name']} records:\", len(recs))\n",
    "    except Exception as e:\n",
    "        print(\"news error\", site[\"name\"], e)\n",
    "\n",
    "df = to_dataframe(all_records)\n",
    "ts = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
    "out_path = f\"storage/raw/crawl_{ts}.csv\"\n",
    "save_csv(df, out_path)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NPR news HTML Crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_npr_listing(html, base_url):\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    items = []\n",
    "    for a in soup.select(\"a[href*='/story/'], a[href*='/sections/world/']\"):\n",
    "        href = a.get(\"href\")\n",
    "        if not href:\n",
    "            continue\n",
    "        url = urlparse.urljoin(base_url, href)\n",
    "        url = normalize_url(url)\n",
    "        if not url:\n",
    "            continue\n",
    "        title = a.get_text(\" \", strip=True) or None\n",
    "        items.append({\n",
    "            \"source_type\": \"news\",\n",
    "            \"source_name\": \"npr_world\",\n",
    "            \"subreddit\": None,\n",
    "            \"url\": url,\n",
    "            \"canonical_url\": url,\n",
    "            \"title\": title,\n",
    "            \"text\": None,\n",
    "            \"author\": None,\n",
    "            \"published_at\": None,\n",
    "            \"score\": None,\n",
    "            \"comments\": None,\n",
    "            \"fetched_at\": datetime.utcnow().isoformat(timespec=\"seconds\")\n",
    "        })\n",
    "    return dedupe_records(items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_npr_news(entry, cfg, max_article_per_listing=40, min_text_len=300):\n",
    "    ua = cfg[\"user_agent\"]\n",
    "    start_urls = entry[\"start_urls\"]\n",
    "    records = []\n",
    "    seen = set()\n",
    "\n",
    "    for su in start_urls:\n",
    "        # واکشی صفحه لیست NPR\n",
    "        resp = fetch(su, ua,\n",
    "                     timeout=cfg[\"default_timeout_sec\"],\n",
    "                     max_retries=cfg[\"max_retries\"],\n",
    "                     min_delay_sec=cfg[\"per_host_min_delay_sec\"])\n",
    "\n",
    "        # لینک ها و شاید عنوان اولیه\n",
    "        listing_items = parse_npr_listing(resp.text, su)\n",
    "\n",
    "        # اگر limit_per_section در کانفیگ بود اعمال کن\n",
    "        limit = entry.get(\"limit_per_section\")\n",
    "        if limit:\n",
    "            listing_items = listing_items[: int(limit)]\n",
    "\n",
    "        for it in listing_items:\n",
    "            lk = it.get(\"url\")\n",
    "            if not lk:\n",
    "                continue\n",
    "            try:\n",
    "                art = fetch(lk, ua,\n",
    "                            timeout=cfg[\"default_timeout_sec\"],\n",
    "                            max_retries=cfg[\"max_retries\"],\n",
    "                            min_delay_sec=cfg[\"per_host_min_delay_sec\"])\n",
    "\n",
    "                # متن\n",
    "                text = extract_article_text(art.text) or \"\"\n",
    "                if len(text) < min_text_len:\n",
    "                    continue\n",
    "\n",
    "                # متادیتا\n",
    "                title, author, published_at, canonical = extract_article_meta(art.text, lk)\n",
    "                canonical = canonical or normalize_url(lk)\n",
    "                if not canonical:\n",
    "                    continue\n",
    "\n",
    "                h = sha1(canonical.encode(\"utf-8\")).hexdigest()\n",
    "                if h in seen:\n",
    "                    continue\n",
    "                seen.add(h)\n",
    "\n",
    "                records.append({\n",
    "                    \"source_type\": \"news\",\n",
    "                    \"source_name\": entry.get(\"name\", \"npr_world\"),\n",
    "                    \"subreddit\": None,\n",
    "                    \"url\": lk,\n",
    "                    \"canonical_url\": canonical,\n",
    "                    \"title\": title or it.get(\"title\"),\n",
    "                    \"text\": text,\n",
    "                    \"author\": author,\n",
    "                    \"published_at\": published_at,\n",
    "                    \"score\": None,\n",
    "                    \"comments\": None,\n",
    "                    \"fetched_at\": datetime.utcnow().isoformat(timespec=\"seconds\")\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(\"error on\", lk, e)\n",
    "    return records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "robots disallows HTML for https://old.reddit.com/r/worldnews/, switching to RSS\n",
      "robots disallows RSS for https://www.reddit.com/r/worldnews/.rss\n",
      "robots disallows HTML for https://old.reddit.com/r/news/, switching to RSS\n",
      "robots disallows RSS for https://www.reddit.com/r/news/.rss\n",
      "robots disallows HTML for https://old.reddit.com/r/politics/, switching to RSS\n",
      "robots disallows RSS for https://www.reddit.com/r/politics/.rss\n",
      "robots disallows HTML for https://old.reddit.com/r/technology/, switching to RSS\n",
      "robots disallows RSS for https://www.reddit.com/r/technology/.rss\n",
      "robots disallows HTML for https://old.reddit.com/r/science/, switching to RSS\n",
      "robots disallows RSS for https://www.reddit.com/r/science/.rss\n",
      "robots disallows HTML for https://old.reddit.com/r/environment/, switching to RSS\n",
      "robots disallows RSS for https://www.reddit.com/r/environment/.rss\n",
      "robots disallows HTML for https://old.reddit.com/r/economics/, switching to RSS\n",
      "robots disallows RSS for https://www.reddit.com/r/economics/.rss\n",
      "skip consent page: https://www.theguardian.com/world/ukraine\n",
      "skip consent page: https://www.theguardian.com/world/europe-news\n",
      "skip consent page: https://www.theguardian.com/world/americas\n",
      "skip consent page: https://www.theguardian.com/world/asia\n",
      "skip consent page: https://www.theguardian.com/world/africa\n",
      "skip consent page: https://www.theguardian.com/world/asia-pacific\n",
      "skip consent page: https://www.theguardian.com/world/south-and-central-asia\n",
      "skip consent page: https://www.bbc.com/news/world\n",
      "skip consent page: https://www.bbc.com/news/us-canada\n",
      "skip consent page: https://www.bbc.com/news/politics\n",
      "skip consent page: https://www.bbc.com/news/world/europe\n",
      "skip consent page: https://www.bbc.com/news/world/latin_america\n",
      "skip consent page: https://www.bbc.com/news/bbcindepth\n",
      "skip consent page: https://www.bbc.com/news/bbcverify\n",
      "Successfully done!\n",
      "saved storage/raw/crawl_20250911_160858.csv\n"
     ]
    }
   ],
   "source": [
    "# ==== Run the crawlers ====\n",
    "with open(\"configs/crawl_config.yaml\", \"r\", encoding=\"utf-8\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "all_records = []\n",
    "\n",
    "# Reddit  اگر در کانفیگ باشد اجرا می شود\n",
    "if \"reddit\" in cfg[\"sources\"] and cfg[\"sources\"][\"reddit\"].get(\"subreddits\"):\n",
    "    for sub in cfg[\"sources\"][\"reddit\"][\"subreddits\"]:\n",
    "        try:\n",
    "            recs = crawl_reddit_subreddit(sub, cfg)\n",
    "            all_records.extend(recs)\n",
    "            # print(f\"reddit {sub} records:\", len(recs))\n",
    "        except Exception as e:\n",
    "            print(\"reddit error\", sub, e)\n",
    "\n",
    "# NPR  همان نقشی که برای جایگزینی Reddit گفتیم\n",
    "if \"npr_news\" in cfg[\"sources\"]:\n",
    "    try:\n",
    "        recs = crawl_npr_news(cfg[\"sources\"][\"npr_news\"], cfg)\n",
    "        all_records.extend(recs)\n",
    "        # print(\"npr_news records:\", len(recs))\n",
    "    except Exception as e:\n",
    "        print(\"npr_news error\", e)\n",
    "\n",
    "# News sites\n",
    "for site in cfg[\"sources\"].get(\"news_sites\", []):\n",
    "    try:\n",
    "        recs = crawl_news_site(site, cfg)\n",
    "        all_records.extend(recs)\n",
    "        # print(f\"news {site['name']} records:\", len(recs))\n",
    "    except Exception as e:\n",
    "        print(\"news error\", site[\"name\"], e)\n",
    "\n",
    "print(\"Successfully done!\")\n",
    "# یکتا سازی سراسری\n",
    "all_records = dedupe_records(all_records)\n",
    "\n",
    "df = to_dataframe(all_records)\n",
    "ts = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
    "out_path = f\"storage/raw/crawl_{ts}.csv\"\n",
    "save_csv(df, out_path)\n",
    "# df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Text Preproccessing</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Text preprocessing utilities ===\n",
    "import re, json, numpy as np, pandas as pd\n",
    "from typing import List\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URL_RE = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "HTML_TAG_RE = re.compile(r\"<[^>]+>\")\n",
    "NON_WORD_RE = re.compile(r\"[^a-zA-Z]+\")   # اگر زبان های دیگر داری بعدا گسترش بده\n",
    "WS_RE = re.compile(r\"\\s+\")\n",
    "\n",
    "def clean_text_basic(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    t = text\n",
    "    t = BeautifulSoup(t, \"lxml\").get_text(\" \", strip=True)  # حذف تگ ها اگر مانده باشد\n",
    "    t = URL_RE.sub(\" \", t)\n",
    "    t = HTML_TAG_RE.sub(\" \", t)\n",
    "    t = t.lower()\n",
    "    t = NON_WORD_RE.sub(\" \", t)\n",
    "    t = WS_RE.sub(\" \", t).strip()\n",
    "    return t\n",
    "\n",
    "def token_count(s: str) -> int:\n",
    "    if not s:\n",
    "        return 0\n",
    "    return len(s.split())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>TF-IDF Model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reuters model not built: Reuters SGML files not found. Put reut2-*.sgm in data/reuters21578\n"
     ]
    }
   ],
   "source": [
    "# === Build TF-IDF model from Reuters-21578 ===\n",
    "import os, glob\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "REUTERS_DIR = \"data/reuters21578\"   # مسیر پوشه ای که فایل های reut2-*.sgm در آن قرار دارد\n",
    "TOP_N_CATEGORIES = 8                 # تعداد دسته ثابت\n",
    "NGRAM_RANGE = (1, 2)\n",
    "MIN_DF = 3\n",
    "MAX_DF = 0.8\n",
    "TOP_K_PER_CLASS = 300                # تعداد n-gram برتر هر دسته\n",
    "\n",
    "def load_reuters_sgml(reuters_dir: str):\n",
    "    docs, labels = [], []\n",
    "    files = sorted(glob.glob(os.path.join(reuters_dir, \"reut2-*.sgm\")))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(\"Reuters SGML files not found. Put reut2-*.sgm in data/reuters21578\")\n",
    "    for fp in files:\n",
    "        with open(fp, \"r\", encoding=\"latin-1\") as f:\n",
    "            raw = f.read()\n",
    "        # برش ساده روی تگ REUTERS\n",
    "        parts = re.split(r\"</REUTERS>\", raw, flags=re.IGNORECASE)\n",
    "        for part in parts:\n",
    "            if \"<REUTERS\" not in part:\n",
    "                continue\n",
    "            # عنوان و بدنه\n",
    "            title = re.search(r\"<TITLE>(.*?)</TITLE>\", part, flags=re.S|re.I)\n",
    "            body = re.search(r\"<BODY>(.*?)</BODY>\", part, flags=re.S|re.I)\n",
    "            text = \"\"\n",
    "            if title:\n",
    "                text += title.group(1) + \" \"\n",
    "            if body:\n",
    "                text += body.group(1)\n",
    "            text = clean_text_basic(text)\n",
    "            if len(text) < 10:\n",
    "                continue\n",
    "            # برچسب ها\n",
    "            topics_block = re.search(r\"<TOPICS>(.*?)</TOPICS>\", part, flags=re.S|re.I)\n",
    "            labs = []\n",
    "            if topics_block:\n",
    "                labs = re.findall(r\"<D>(.*?)</D>\", topics_block.group(1), flags=re.I)\n",
    "            if text:\n",
    "                docs.append(text)\n",
    "                labels.append(labs)\n",
    "    return docs, labels\n",
    "\n",
    "def build_reuters_model(reuters_dir: str,\n",
    "                        top_n_categories=TOP_N_CATEGORIES,\n",
    "                        ngram_range=NGRAM_RANGE,\n",
    "                        min_df=MIN_DF,\n",
    "                        max_df=MAX_DF,\n",
    "                        top_k_per_class=TOP_K_PER_CLASS):\n",
    "    docs, labels = load_reuters_sgml(reuters_dir)\n",
    "    # شمارش فراوانی دسته ها\n",
    "    c = Counter([lab for lablist in labels for lab in lablist])\n",
    "    top_cats = [k for k, _ in c.most_common(top_n_categories)]\n",
    "\n",
    "    # نگه داشتن فقط اسنادی که حداقل یکی از این دسته ها را دارند\n",
    "    sel_docs, sel_targets = [], []\n",
    "    for t, labs in zip(docs, labels):\n",
    "        inter = [lb for lb in labs if lb in top_cats]\n",
    "        if inter:\n",
    "            sel_docs.append(t)\n",
    "            sel_targets.append(inter[0])  # یک برچسب غالب، ساده و کافی برای وزن دهی\n",
    "\n",
    "    if not sel_docs:\n",
    "        raise RuntimeError(\"No Reuters docs selected. Check dataset path or top_n_categories.\")\n",
    "\n",
    "    vectorizer = TfidfVectorizer(ngram_range=ngram_range,\n",
    "                                 min_df=min_df,\n",
    "                                 max_df=max_df,\n",
    "                                 stop_words=\"english\")\n",
    "    X = vectorizer.fit_transform(sel_docs)\n",
    "    vocab = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "    # محاسبه وزن میانگین هر ویژگی برای هر دسته\n",
    "    class_centroids = {}\n",
    "    class_top_lexicons = {}\n",
    "    for cat in top_cats:\n",
    "        idx = [i for i, y in enumerate(sel_targets) if y == cat]\n",
    "        if not idx:\n",
    "            continue\n",
    "        mean_vec = X[idx].mean(axis=0).A1   # بردار میانگین\n",
    "        class_centroids[cat] = mean_vec\n",
    "        top_idx = mean_vec.argsort()[::-1][:top_k_per_class]\n",
    "        class_top_lexicons[cat] = set(top_idx.tolist())\n",
    "\n",
    "    model = {\n",
    "        \"vectorizer\": vectorizer,\n",
    "        \"vocab\": vocab,\n",
    "        \"top_categories\": top_cats,\n",
    "        \"centroids\": class_centroids,\n",
    "        \"top_lexicons\": class_top_lexicons\n",
    "    }\n",
    "    return model\n",
    "\n",
    "# ساخت مدل\n",
    "try:\n",
    "    REUTERS_MODEL = build_reuters_model(REUTERS_DIR)\n",
    "    print(\"Reuters model ready. Categories:\", REUTERS_MODEL[\"top_categories\"])\n",
    "except Exception as e:\n",
    "    print(\"Reuters model not built:\", e)\n",
    "    REUTERS_MODEL = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Rating and labeling on our data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Scoring and labeling ===\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def score_article(text: str, model) -> dict:\n",
    "    vec = model[\"vectorizer\"].transform([text])\n",
    "    scores = {}\n",
    "    for cat in model[\"top_categories\"]:\n",
    "        centroid = model[\"centroids\"][cat]\n",
    "        top_idx = list(model[\"top_lexicons\"][cat])\n",
    "        # ضرب داخلی فقط روی n-gram های برتر\n",
    "        x_sub = vec[:, top_idx]\n",
    "        c_sub = centroid[top_idx]\n",
    "        score = float(x_sub.multiply(c_sub).sum())\n",
    "        scores[cat] = score\n",
    "    return scores\n",
    "\n",
    "def label_articles_dataframe(df: pd.DataFrame, model, text_col=\"text\"):\n",
    "    preds, score_jsons, langs, tok_counts = [], [], [], []\n",
    "    for t in df[text_col].fillna(\"\"):\n",
    "        ct = clean_text_basic(t)\n",
    "        tok_counts.append(token_count(ct))\n",
    "        if not ct or not model:\n",
    "            preds.append(None)\n",
    "            score_jsons.append(None)\n",
    "            langs.append(None)\n",
    "            continue\n",
    "        sc = score_article(ct, model)\n",
    "        pred = max(sc, key=sc.get) if sc else None\n",
    "        preds.append(pred)\n",
    "        score_jsons.append(json.dumps(sc))\n",
    "        langs.append(\"en\")  # اگر زبانشناسی اضافه کردی اینجا جایگزین کن\n",
    "    df[\"language\"] = langs\n",
    "    df[\"token_count\"] = tok_counts\n",
    "    df[\"predicted_label\"] = preds\n",
    "    df[\"label_scores\"] = score_jsons\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Labeling on crawled data </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved storage/clean/news_labeled_20250911_162433.csv\n"
     ]
    }
   ],
   "source": [
    "# === Load crawl CSV, label, and save ===\n",
    "# اگر همین حالا df را در حافظه داری می توانی همان را بدهی\n",
    "# در غیر این صورت آخرین فایل را از storage/raw بخوان\n",
    "import glob\n",
    "\n",
    "def latest_crawl_csv():\n",
    "    files = sorted(glob.glob(\"storage/raw/crawl_*.csv\"))\n",
    "    return files[-1] if files else None\n",
    "\n",
    "if 'df' not in globals() or df is None:\n",
    "    path = latest_crawl_csv()\n",
    "    if path:\n",
    "        df = pd.read_csv(path)\n",
    "        print(\"loaded\", path, \"rows:\", len(df))\n",
    "    else:\n",
    "        raise RuntimeError(\"No crawl CSV found in storage/raw\")\n",
    "\n",
    "# پر کردن ستون های پیش بینی با مدل ساخته شده\n",
    "# if REUTERS_MODEL is None:\n",
    "#     print(\"Warning, Reuters model is missing, skip labeling\")\n",
    "# else:\n",
    "df = label_articles_dataframe(df, REUTERS_MODEL, text_col=\"text\")\n",
    "\n",
    "# پیش‌پردازش سبک روی متن\n",
    "df[\"clean_text\"] = df[\"text\"].fillna(\"\").apply(clean_text_basic)\n",
    "df[\"token_count\"] = df[\"clean_text\"].apply(token_count)\n",
    "df[\"language\"] = \"en\"  # اگر تشخیص زبان اضافه نکرده‌ای\n",
    "\n",
    "# اگر مدل Reuters را ساخته‌ای، برچسب را هم همینجا پر کن\n",
    "if REUTERS_MODEL is not None:\n",
    "    df = label_articles_dataframe(df, REUTERS_MODEL, text_col=\"text\")\n",
    "\n",
    "\n",
    "# ذخیره خروجی در پوشه clean\n",
    "ts = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
    "out_clean = f\"storage/clean/news_labeled_{ts}.csv\"\n",
    "save_csv(df, out_clean)\n",
    "# df.head(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
