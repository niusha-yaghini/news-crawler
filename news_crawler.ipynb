{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> imports <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, threading, random, re, urllib.parse as urlparse, yaml, requests, pandas as pd\n",
    "from collections import defaultdict\n",
    "from urllib.robotparser import RobotFileParser\n",
    "from bs4 import BeautifulSoup\n",
    "from hashlib import sha1\n",
    "import tldextract\n",
    "from datetime import datetime\n",
    "from readability import Document\n",
    "import feedparser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 1.\tCrawler / Scraper <h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining basic settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configs/crawl_config.yaml saved\n"
     ]
    }
   ],
   "source": [
    "# os.makedirs(\"configs\", exist_ok=True)\n",
    "# os.makedirs(\"storage/raw\", exist_ok=True)\n",
    "# os.makedirs(\"storage/clean\", exist_ok=True)\n",
    "# os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "# CONFIG = {\n",
    "#     \"user_agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36\",\n",
    "#     \"default_timeout_sec\": 15,\n",
    "#     \"per_host_min_delay_sec\": 2,\n",
    "#     \"max_retries\": 3,\n",
    "#     \"sources\": {\n",
    "#         \"reddit\": {\n",
    "#             \"type\": \"reddit_html\",\n",
    "#             \"subreddits\": [\"worldnews\", \"news\"],\n",
    "#             \"limit_per_sub\": 0\n",
    "#         },\n",
    "#         \"news_sites\": [\n",
    "#             {\n",
    "#                 \"name\": \"reuters_world\",\n",
    "#                 \"start_urls\": [\"https://www.reuters.com/world/\"],\n",
    "#                 \"article_selector_hint\": \"a\"\n",
    "#             }\n",
    "#         ]\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# with open(\"configs/crawl_config.yaml\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     yaml.safe_dump(CONFIG, f, allow_unicode=True)\n",
    "# print(\"configs/crawl_config.yaml saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, yaml\n",
    "\n",
    "os.makedirs(\"configs\", exist_ok=True)\n",
    "os.makedirs(\"storage/raw\", exist_ok=True)\n",
    "os.makedirs(\"storage/clean\", exist_ok=True)\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "CONFIG = {\n",
    "    \"user_agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36\",\n",
    "    \"default_timeout_sec\": 15,\n",
    "    \"per_host_min_delay_sec\": 2,\n",
    "    \"max_retries\": 3,\n",
    "    \"sources\": {\n",
    "        \"reddit\": {\n",
    "            \"type\": \"reddit_html\",\n",
    "            \"subreddits\": [\n",
    "                \"worldnews\",\n",
    "                \"news\",\n",
    "                \"politics\",\n",
    "                \"technology\",\n",
    "                \"science\",\n",
    "                \"environment\",\n",
    "                \"economics\"\n",
    "            ],\n",
    "            \"limit_per_sub\": 50\n",
    "        },\n",
    "        \"npr_news\": {\n",
    "            \"type\": \"news_html\",\n",
    "            \"start_urls\": [\"https://www.npr.org/sections/world/\"],\n",
    "            \"article_selector_hint\": \"a\",\n",
    "            \"include_patterns\": [\"/story/\", \"/202\", \"/sections/world/\"],\n",
    "            \"limit_per_section\": 50\n",
    "        },\n",
    "        \"news_sites\": [\n",
    "            {\n",
    "                \"name\": \"ap_world\",\n",
    "                \"start_urls\": [\"https://apnews.com/hub/world-news\"],\n",
    "                \"article_selector_hint\": \"a\"\n",
    "            }\n",
    "        ]\n",
    "        # \"news_sites\": [\n",
    "        #     {\n",
    "        #         \"name\": \"reuters_world\",\n",
    "        #         \"start_urls\": [\"https://www.reuters.com/world/\"],\n",
    "        #         \"article_selector_hint\": \"a\"\n",
    "        #     }\n",
    "        # ]\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(\"configs/crawl_config.yaml\", \"w\", encoding=\"utf-8\") as f:\n",
    "    yaml.safe_dump(CONFIG, f, allow_unicode=True)\n",
    "print(\"configs/crawl_config.yaml saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the robots.txt and control the rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "_session = requests.Session()\n",
    "# _session.headers.update({\"Accept-Language\": \"en;q=0.9\"})\n",
    "_session.headers.update({\n",
    "    \"Accept-Language\": \"en;q=0.9\",\n",
    "    \"User-Agent\": CONFIG[\"user_agent\"],\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "    \"Connection\": \"keep-alive\"\n",
    "})\n",
    "\n",
    "# ==== robots.txt handling and rate limit ====\n",
    "_robot_cache = {}\n",
    "_host_next_time = defaultdict(float)\n",
    "_lock = threading.Lock()\n",
    "\n",
    "def get_robots_parser(base_url, ua):\n",
    "    parsed = urlparse.urlparse(base_url)\n",
    "    robots_url = f\"{parsed.scheme}://{parsed.netloc}/robots.txt\"\n",
    "    if robots_url in _robot_cache:\n",
    "        return _robot_cache[robots_url]\n",
    "    rp = RobotFileParser()\n",
    "    try:\n",
    "        resp = _session.get(robots_url, timeout=8)\n",
    "        if resp.status_code == 200:\n",
    "            rp.parse(resp.text.splitlines())\n",
    "        else:\n",
    "            rp.parse([])\n",
    "    except Exception:\n",
    "        rp.parse([])\n",
    "    _robot_cache[robots_url] = rp\n",
    "    return rp\n",
    "\n",
    "def host_key(url):\n",
    "    p = urlparse.urlparse(url)\n",
    "    ext = tldextract.extract(p.netloc)\n",
    "    return \".\".join([x for x in [ext.domain, ext.suffix] if x])\n",
    "\n",
    "def rate_limit(url, min_delay_sec):\n",
    "    hk = host_key(url)\n",
    "    with _lock:\n",
    "        now = time.time()\n",
    "        nt = _host_next_time[hk]\n",
    "        wait = nt - now\n",
    "        if wait > 0:\n",
    "            time.sleep(wait)\n",
    "        _host_next_time[hk] = time.time() + min_delay_sec\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch function (with Retry & Exponential Backoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch(url, user_agent, timeout=15, max_retries=3, min_delay_sec=2):\n",
    "    rp = get_robots_parser(url, user_agent)\n",
    "    if not rp.can_fetch(user_agent, url):\n",
    "        raise PermissionError(f\"Blocked by robots.txt for {url}\")\n",
    "    rate_limit(url, min_delay_sec)\n",
    "    headers = {\"User-Agent\": user_agent}\n",
    "    attempt = 0\n",
    "    backoff = 1.6\n",
    "    while attempt <= max_retries:\n",
    "        try:\n",
    "            resp = _session.get(url, headers=headers, timeout=timeout)\n",
    "            if 200 <= resp.status_code < 300:\n",
    "                return resp\n",
    "            if resp.status_code in (429, 503):\n",
    "                time.sleep((backoff ** attempt) + random.uniform(0, 0.5))\n",
    "            else:\n",
    "                time.sleep(0.6)\n",
    "        except requests.RequestException:\n",
    "            time.sleep((backoff ** attempt) + 0.4)\n",
    "        attempt += 1\n",
    "    raise TimeoutError(f\"Fetch failed after retries for {url}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML Crawler for Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "UPVOTE_RE = re.compile(r\"(\\d+(?:\\.\\d+)?)([kK])?\\s*upvote\")\n",
    "\n",
    "def to_int_k(v):\n",
    "    if v is None:\n",
    "        return None\n",
    "    s = str(v).strip().lower()\n",
    "    if s.endswith(\"k\"):\n",
    "        try:\n",
    "            return int(float(s[:-1]) * 1000)\n",
    "        except:\n",
    "            return None\n",
    "    try:\n",
    "        return int(s)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parse_reddit_listing(html, base_url):\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    items = []\n",
    "    for post in soup.select(\"[data-testid='post-container'], div[data-test-id='post-content']\"):\n",
    "        title_el = post.select_one(\"h3\") or post.select_one(\"a[data-click-id='body'] h3\")\n",
    "        if not title_el:\n",
    "            continue\n",
    "        title = title_el.get_text(strip=True)\n",
    "        link_el = post.select_one(\"a[data-click-id='body']\") or post.find(\"a\", href=True)\n",
    "        url = urlparse.urljoin(base_url, link_el[\"href\"]) if link_el and link_el.get(\"href\") else None\n",
    "        author_el = post.select_one(\"a[data-click-id='user']\") or post.select_one(\"a[href^='/user/']\")\n",
    "        author = author_el.get_text(strip=True) if author_el else None\n",
    "        time_el = post.select_one(\"a[data-click-id='timestamp'] time\") or post.find(\"time\")\n",
    "        published_at = time_el.get(\"datetime\") if time_el and time_el.has_attr(\"datetime\") else None\n",
    "        comments_el = post.select_one(\"a[data-click-id='comments']\") or post.find(\"a\", string=re.compile(\"comment\", re.I))\n",
    "        comments = None\n",
    "        if comments_el:\n",
    "            m = re.search(r\"(\\d+(?:\\.\\d+)?[kK]?)\", comments_el.get_text(\" \", strip=True))\n",
    "            if m:\n",
    "                comments = to_int_k(m.group(1))\n",
    "        score = None\n",
    "        aria_up = post.find(attrs={\"aria-label\": re.compile(\"upvote\", re.I)})\n",
    "        if aria_up:\n",
    "            m = UPVOTE_RE.search(aria_up.get(\"aria-label\", \"\"))\n",
    "            if m:\n",
    "                val = float(m.group(1))\n",
    "                if m.group(2):\n",
    "                    val *= 1000\n",
    "                score = int(val)\n",
    "        items.append({\n",
    "            \"source_type\": \"reddit\",\n",
    "            \"source_name\": base_url,\n",
    "            \"subreddit\": base_url.rstrip(\"/\").split(\"/\")[-1],\n",
    "            \"url\": url,\n",
    "            \"canonical_url\": url,\n",
    "            \"title\": title,\n",
    "            \"text\": None,\n",
    "            \"author\": author,\n",
    "            \"published_at\": published_at,\n",
    "            \"score\": score,\n",
    "            \"comments\": comments,\n",
    "            \"fetched_at\": datetime.utcnow().isoformat(timespec=\"seconds\")\n",
    "        })\n",
    "    return items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "STRIP_PARAMS = {\"utm_source\",\"utm_medium\",\"utm_campaign\",\"utm_term\",\"utm_content\",\"ref\",\"utm_name\",\"gclid\",\"fbclid\"}\n",
    "\n",
    "def normalize_url(u: str) -> str | None:\n",
    "    if not u:\n",
    "        return None\n",
    "    p = urlparse.urlparse(u)\n",
    "    if p.scheme not in (\"http\",\"https\"):\n",
    "        return None\n",
    "    q = [(k,v) for k,v in urlparse.parse_qsl(p.query, keep_blank_values=True) if k not in STRIP_PARAMS]\n",
    "    new_q = urlparse.urlencode(q)\n",
    "    return urlparse.urlunparse((p.scheme, p.netloc, p.path, \"\", new_q, \"\"))\n",
    "\n",
    "def dedupe_records(records):\n",
    "    seen = set()\n",
    "    unique = []\n",
    "    for r in records:\n",
    "        key = normalize_url(r.get(\"canonical_url\") or r.get(\"url\")) or r.get(\"url\")\n",
    "        if not key:\n",
    "            continue\n",
    "        h = sha1(key.encode(\"utf-8\")).hexdigest()\n",
    "        if h in seen:\n",
    "            continue\n",
    "        seen.add(h)\n",
    "        r[\"canonical_url\"] = key\n",
    "        unique.append(r)\n",
    "    return unique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reddit_rss_url(sub: str) -> str:\n",
    "    # RSS پایدار روی www.reddit.com بهتر جواب می‌دهد\n",
    "    return f\"https://www.reddit.com/r/{sub}/.rss\"\n",
    "\n",
    "def crawl_reddit_via_rss(sub, cfg, limit=None):\n",
    "    rss = reddit_rss_url(sub)\n",
    "    ua = cfg[\"user_agent\"]\n",
    "\n",
    "    # robots برای خود مسیر RSS چک شود\n",
    "    rp = get_robots_parser(rss, ua)\n",
    "    if not rp.can_fetch(ua, rss):\n",
    "        print(f\"robots disallows RSS for {rss}\")\n",
    "        return []\n",
    "\n",
    "    # با Session خودمان fetch کنیم تا UA درست ارسال شود\n",
    "    resp = fetch(rss, ua,\n",
    "                 timeout=cfg[\"default_timeout_sec\"],\n",
    "                 max_retries=cfg[\"max_retries\"],\n",
    "                 min_delay_sec=cfg[\"per_host_min_delay_sec\"])\n",
    "\n",
    "    print(\"rss status:\", resp.status_code, \"bytes:\", len(resp.text))\n",
    "\n",
    "    d = feedparser.parse(resp.text)\n",
    "    print(\"rss entries:\", len(d.get(\"entries\", [])))\n",
    "    items = []\n",
    "    now = datetime.utcnow().isoformat(timespec=\"seconds\")\n",
    "    for e in d.get(\"entries\", [])[: (limit or 50)]:\n",
    "        link = e.get(\"link\")\n",
    "        items.append({\n",
    "            \"source_type\": \"reddit\",\n",
    "            \"source_name\": rss,\n",
    "            \"subreddit\": sub,\n",
    "            \"url\": link,\n",
    "            \"canonical_url\": normalize_url(link) if link else link,\n",
    "            \"title\": e.get(\"title\"),\n",
    "            \"text\": None,\n",
    "            \"author\": e.get(\"author\") if \"author\" in e else None,\n",
    "            \"published_at\": e.get(\"published\") if \"published\" in e else None,\n",
    "            \"score\": None,\n",
    "            \"comments\": None,\n",
    "            \"fetched_at\": now\n",
    "        })\n",
    "    return dedupe_records(items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_reddit_subreddit(sub, cfg):\n",
    "    ua = cfg[\"user_agent\"]\n",
    "    limit = cfg[\"sources\"][\"reddit\"][\"limit_per_sub\"]\n",
    "    html_url = f\"https://old.reddit.com/r/{sub}/\"\n",
    "    rp = get_robots_parser(html_url, ua)\n",
    "    if not rp.can_fetch(ua, html_url):\n",
    "        print(f\"robots disallows HTML for {html_url}, switching to RSS\")\n",
    "        return crawl_reddit_via_rss(sub, cfg, limit)\n",
    "    # اگر HTML مجاز بود همان مسیر قبلی\n",
    "    resp = fetch(html_url, ua,\n",
    "                 timeout=cfg[\"default_timeout_sec\"],\n",
    "                 max_retries=cfg[\"max_retries\"],\n",
    "                 min_delay_sec=cfg[\"per_host_min_delay_sec\"])\n",
    "    items = parse_reddit_listing(resp.text, html_url)\n",
    "    items = dedupe_records(items)\n",
    "    return items[:limit] if limit else items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Article text extraction ====\n",
    "def extract_article_text(html):\n",
    "    try:\n",
    "        doc = Document(html)\n",
    "        content_html = doc.summary()\n",
    "        soup = BeautifulSoup(content_html, \"lxml\")\n",
    "        text = soup.get_text(\"\\n\", strip=True)\n",
    "        if len(text) < 200:\n",
    "            soup_full = BeautifulSoup(html, \"lxml\")\n",
    "            paras = [p.get_text(\" \", strip=True) for p in soup_full.select(\"p\")]\n",
    "            text = \"\\n\".join(paras[:60])\n",
    "        return text\n",
    "    except Exception:\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        paras = [p.get_text(\" \", strip=True) for p in soup.select(\"p\")]\n",
    "        return \"\\n\".join(paras[:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Link discovery for news sites ====\n",
    "def parse_listing_find_links(html, base_url, selector_hint=\"a\"):\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    base = urlparse.urlparse(base_url).netloc\n",
    "    links = []\n",
    "    for a in soup.select(selector_hint):\n",
    "        href = a.get(\"href\")\n",
    "        if not href:\n",
    "            continue\n",
    "        full = urlparse.urljoin(base_url, href)\n",
    "        norm = normalize_url(full)\n",
    "        if not norm:\n",
    "            continue\n",
    "        net = urlparse.urlparse(norm).netloc\n",
    "        if net != base:\n",
    "            continue\n",
    "        if norm.endswith(\"#\"):\n",
    "            norm = norm[:-1]\n",
    "        links.append(norm)\n",
    "    return list(dict.fromkeys(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Consent page detection ====\n",
    "# def looks_like_consent_page(html: str) -> bool:\n",
    "#     s = html.lower()\n",
    "#     return (\"consent\" in s) or (\"gdpr\" in s) or (\"privacy preferences\" in s) or (\"iab\" in s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Extract article meta (title, author, published_at, canonical) ====\n",
    "def extract_article_meta(html: str, url: str):\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    title = None\n",
    "    for sel in [\"meta[property='og:title']\", \"meta[name='twitter:title']\", \"title\"]:\n",
    "        el = soup.select_one(sel)\n",
    "        if el:\n",
    "            title = el.get(\"content\") if el.has_attr(\"content\") else el.get_text(strip=True)\n",
    "        if title:\n",
    "            break\n",
    "    author = None\n",
    "    for sel in [\"meta[name='author']\", \"meta[property='article:author']\", \"a[rel='author']\"]:\n",
    "        el = soup.select_one(sel)\n",
    "        if el:\n",
    "            author = el.get(\"content\") if el.has_attr(\"content\") else el.get_text(strip=True)\n",
    "        if author:\n",
    "            break\n",
    "    published_at = None\n",
    "    for sel in [\"meta[property='article:published_time']\", \"meta[name='pubdate']\", \"time[datetime]\"]:\n",
    "        el = soup.select_one(sel)\n",
    "        if el:\n",
    "            published_at = el.get(\"content\") if el.has_attr(\"content\") else el.get(\"datetime\")\n",
    "        if published_at:\n",
    "            break\n",
    "    can = soup.find(\"link\", rel=lambda x: x and \"canonical\" in x.lower())\n",
    "    canonical = urlparse.urljoin(url, can[\"href\"]) if can and can.get(\"href\") else url\n",
    "    canonical = normalize_url(canonical)\n",
    "    return title, author, published_at, canonical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Crawl news site ====\n",
    "def crawl_news_site(entry, cfg, max_article_per_listing=30, min_text_len=300):\n",
    "    ua = cfg[\"user_agent\"]\n",
    "    start_urls = entry[\"start_urls\"]\n",
    "    selector_hint = entry.get(\"article_selector_hint\", \"a\")\n",
    "    records = []\n",
    "    seen = set()\n",
    "    for su in start_urls:\n",
    "        resp = fetch(su, ua,\n",
    "                     timeout=cfg[\"default_timeout_sec\"],\n",
    "                     max_retries=cfg[\"max_retries\"],\n",
    "                     min_delay_sec=cfg[\"per_host_min_delay_sec\"])\n",
    "        links = parse_listing_find_links(resp.text, su, selector_hint)\n",
    "        for lk in links[:max_article_per_listing]:\n",
    "            try:\n",
    "                art = fetch(lk, ua,\n",
    "                            timeout=cfg[\"default_timeout_sec\"],\n",
    "                            max_retries=cfg[\"max_retries\"],\n",
    "                            min_delay_sec=cfg[\"per_host_min_delay_sec\"])\n",
    "                # if looks_like_consent_page(art.text):\n",
    "                #     print(\"skip consent page:\", lk)\n",
    "                #     continue\n",
    "                text = extract_article_text(art.text) or \"\"\n",
    "                if len(text) < min_text_len:\n",
    "                    continue\n",
    "                title, author, published_at, canonical = extract_article_meta(art.text, lk)\n",
    "                canonical = canonical or normalize_url(lk)\n",
    "                if not canonical:\n",
    "                    continue\n",
    "                h = sha1(canonical.encode(\"utf-8\")).hexdigest()\n",
    "                if h in seen:\n",
    "                    continue\n",
    "                seen.add(h)\n",
    "                records.append({\n",
    "                    \"source_type\": \"news\",\n",
    "                    \"source_name\": entry[\"name\"],\n",
    "                    \"subreddit\": None,\n",
    "                    \"url\": lk,\n",
    "                    \"canonical_url\": canonical,\n",
    "                    \"title\": title,\n",
    "                    \"text\": text,\n",
    "                    \"author\": author,\n",
    "                    \"published_at\": published_at,\n",
    "                    \"score\": None,\n",
    "                    \"comments\": None,\n",
    "                    \"fetched_at\": datetime.utcnow().isoformat(timespec=\"seconds\")\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(\"error on\", lk, e)\n",
    "    return records\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSV Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== CSV Schema and helpers ====\n",
    "CSV_SCHEMA = [\n",
    "    \"source_type\", \"source_name\", \"subreddit\", \"url\", \"canonical_url\",\n",
    "    \"title\", \"text\", \"author\", \"published_at\", \"score\", \"comments\",\n",
    "    \"language\", \"token_count\", \"predicted_label\", \"label_scores\", \"fetched_at\"\n",
    "]\n",
    "\n",
    "def to_dataframe(records):\n",
    "    df = pd.DataFrame(records)\n",
    "    for col in CSV_SCHEMA:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "    return df[CSV_SCHEMA]\n",
    "\n",
    "def save_csv(df, path):\n",
    "    df.to_csv(path, index=False, encoding=\"utf-8\")\n",
    "    print(\"saved\", path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "robots disallows HTML for https://old.reddit.com/r/worldnews/, switching to RSS\n",
      "robots disallows RSS for https://www.reddit.com/r/worldnews/.rss\n",
      "reddit worldnews records: 0\n",
      "robots disallows HTML for https://old.reddit.com/r/news/, switching to RSS\n",
      "robots disallows RSS for https://www.reddit.com/r/news/.rss\n",
      "reddit news records: 0\n",
      "news ap_world records: 16\n",
      "saved storage/raw/crawl_20250911_134126.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_type</th>\n",
       "      <th>source_name</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>url</th>\n",
       "      <th>canonical_url</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>published_at</th>\n",
       "      <th>score</th>\n",
       "      <th>comments</th>\n",
       "      <th>language</th>\n",
       "      <th>token_count</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>label_scores</th>\n",
       "      <th>fetched_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>news</td>\n",
       "      <td>ap_world</td>\n",
       "      <td>None</td>\n",
       "      <td>https://apnews.com/</td>\n",
       "      <td>https://apnews.com/</td>\n",
       "      <td>Associated Press News: Breaking News | Latest ...</td>\n",
       "      <td>\\nCopyright 2025 The Associated Press. All Rig...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-09-11T13:40:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>news</td>\n",
       "      <td>ap_world</td>\n",
       "      <td>None</td>\n",
       "      <td>https://apnews.com/world-news</td>\n",
       "      <td>https://apnews.com/world-news</td>\n",
       "      <td>World News</td>\n",
       "      <td>\\nCopyright 2025 The Associated Press. All Rig...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-09-11T13:40:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>news</td>\n",
       "      <td>ap_world</td>\n",
       "      <td>None</td>\n",
       "      <td>https://apnews.com/hub/china</td>\n",
       "      <td>https://apnews.com/hub/china</td>\n",
       "      <td>China | Latest News from China Today</td>\n",
       "      <td>\\nCopyright 2025 The Associated Press. All Rig...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-09-11T13:40:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>news</td>\n",
       "      <td>ap_world</td>\n",
       "      <td>None</td>\n",
       "      <td>https://apnews.com/article/nato-poland-russia-...</td>\n",
       "      <td>https://apnews.com/article/nato-poland-russia-...</td>\n",
       "      <td>What NATO's Article 4 talks mean after Russian...</td>\n",
       "      <td>BRUSSELS (AP) —\\nNATO\\nallies swiftly held tal...</td>\n",
       "      <td>https://apnews.com/author/the-associated-press</td>\n",
       "      <td>2025-09-10T12:59:34</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-09-11T13:40:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>news</td>\n",
       "      <td>ap_world</td>\n",
       "      <td>None</td>\n",
       "      <td>https://apnews.com/article/prince-harry-king-c...</td>\n",
       "      <td>https://apnews.com/article/prince-harry-king-c...</td>\n",
       "      <td>Prince Harry has tea with his father, King Cha...</td>\n",
       "      <td>LONDON (AP) — Britain’s\\nPrince Harry\\njoined ...</td>\n",
       "      <td>https://apnews.com/author/danica-kirka</td>\n",
       "      <td>2025-09-10T16:52:03</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-09-11T13:40:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>news</td>\n",
       "      <td>ap_world</td>\n",
       "      <td>None</td>\n",
       "      <td>https://apnews.com/article/south-korea-visa-us...</td>\n",
       "      <td>https://apnews.com/article/south-korea-visa-us...</td>\n",
       "      <td>South Korea says detained Korean workers relea...</td>\n",
       "      <td>SEOUL, South Korea (AP) — South Korea’s presid...</td>\n",
       "      <td>https://apnews.com/author/kim-tong-hyung</td>\n",
       "      <td>2025-09-11T02:51:31</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-09-11T13:40:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>news</td>\n",
       "      <td>ap_world</td>\n",
       "      <td>None</td>\n",
       "      <td>https://apnews.com/us-news</td>\n",
       "      <td>https://apnews.com/us-news</td>\n",
       "      <td>U.S. News</td>\n",
       "      <td>\\nCopyright 2025 The Associated Press. All Rig...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-09-11T13:40:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>news</td>\n",
       "      <td>ap_world</td>\n",
       "      <td>None</td>\n",
       "      <td>https://apnews.com/hub/immigration</td>\n",
       "      <td>https://apnews.com/hub/immigration</td>\n",
       "      <td>Immigration</td>\n",
       "      <td>\\nCopyright 2025 The Associated Press. All Rig...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-09-11T13:40:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>news</td>\n",
       "      <td>ap_world</td>\n",
       "      <td>None</td>\n",
       "      <td>https://apnews.com/hub/weather</td>\n",
       "      <td>https://apnews.com/hub/weather</td>\n",
       "      <td>Weather, Hurricanes and Storms | Latest News &amp;...</td>\n",
       "      <td>\\nCopyright 2025 The Associated Press. All Rig...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-09-11T13:40:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>news</td>\n",
       "      <td>ap_world</td>\n",
       "      <td>None</td>\n",
       "      <td>https://apnews.com/hub/abortion</td>\n",
       "      <td>https://apnews.com/hub/abortion</td>\n",
       "      <td>Abortion | Latest News &amp; Updates</td>\n",
       "      <td>\\nCopyright 2025 The Associated Press. All Rig...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-09-11T13:41:03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source_type source_name subreddit  \\\n",
       "0        news    ap_world      None   \n",
       "1        news    ap_world      None   \n",
       "2        news    ap_world      None   \n",
       "3        news    ap_world      None   \n",
       "4        news    ap_world      None   \n",
       "5        news    ap_world      None   \n",
       "6        news    ap_world      None   \n",
       "7        news    ap_world      None   \n",
       "8        news    ap_world      None   \n",
       "9        news    ap_world      None   \n",
       "\n",
       "                                                 url  \\\n",
       "0                                https://apnews.com/   \n",
       "1                      https://apnews.com/world-news   \n",
       "2                       https://apnews.com/hub/china   \n",
       "3  https://apnews.com/article/nato-poland-russia-...   \n",
       "4  https://apnews.com/article/prince-harry-king-c...   \n",
       "5  https://apnews.com/article/south-korea-visa-us...   \n",
       "6                         https://apnews.com/us-news   \n",
       "7                 https://apnews.com/hub/immigration   \n",
       "8                     https://apnews.com/hub/weather   \n",
       "9                    https://apnews.com/hub/abortion   \n",
       "\n",
       "                                       canonical_url  \\\n",
       "0                                https://apnews.com/   \n",
       "1                      https://apnews.com/world-news   \n",
       "2                       https://apnews.com/hub/china   \n",
       "3  https://apnews.com/article/nato-poland-russia-...   \n",
       "4  https://apnews.com/article/prince-harry-king-c...   \n",
       "5  https://apnews.com/article/south-korea-visa-us...   \n",
       "6                         https://apnews.com/us-news   \n",
       "7                 https://apnews.com/hub/immigration   \n",
       "8                     https://apnews.com/hub/weather   \n",
       "9                    https://apnews.com/hub/abortion   \n",
       "\n",
       "                                               title  \\\n",
       "0  Associated Press News: Breaking News | Latest ...   \n",
       "1                                         World News   \n",
       "2               China | Latest News from China Today   \n",
       "3  What NATO's Article 4 talks mean after Russian...   \n",
       "4  Prince Harry has tea with his father, King Cha...   \n",
       "5  South Korea says detained Korean workers relea...   \n",
       "6                                          U.S. News   \n",
       "7                                        Immigration   \n",
       "8  Weather, Hurricanes and Storms | Latest News &...   \n",
       "9                  Abortion | Latest News & Updates    \n",
       "\n",
       "                                                text  \\\n",
       "0  \\nCopyright 2025 The Associated Press. All Rig...   \n",
       "1  \\nCopyright 2025 The Associated Press. All Rig...   \n",
       "2  \\nCopyright 2025 The Associated Press. All Rig...   \n",
       "3  BRUSSELS (AP) —\\nNATO\\nallies swiftly held tal...   \n",
       "4  LONDON (AP) — Britain’s\\nPrince Harry\\njoined ...   \n",
       "5  SEOUL, South Korea (AP) — South Korea’s presid...   \n",
       "6  \\nCopyright 2025 The Associated Press. All Rig...   \n",
       "7  \\nCopyright 2025 The Associated Press. All Rig...   \n",
       "8  \\nCopyright 2025 The Associated Press. All Rig...   \n",
       "9  \\nCopyright 2025 The Associated Press. All Rig...   \n",
       "\n",
       "                                           author         published_at score  \\\n",
       "0                                            None                 None  None   \n",
       "1                                            None                 None  None   \n",
       "2                                            None                 None  None   \n",
       "3  https://apnews.com/author/the-associated-press  2025-09-10T12:59:34  None   \n",
       "4          https://apnews.com/author/danica-kirka  2025-09-10T16:52:03  None   \n",
       "5        https://apnews.com/author/kim-tong-hyung  2025-09-11T02:51:31  None   \n",
       "6                                            None                 None  None   \n",
       "7                                            None                 None  None   \n",
       "8                                            None                 None  None   \n",
       "9                                            None                 None  None   \n",
       "\n",
       "  comments language token_count predicted_label label_scores  \\\n",
       "0     None     None        None            None         None   \n",
       "1     None     None        None            None         None   \n",
       "2     None     None        None            None         None   \n",
       "3     None     None        None            None         None   \n",
       "4     None     None        None            None         None   \n",
       "5     None     None        None            None         None   \n",
       "6     None     None        None            None         None   \n",
       "7     None     None        None            None         None   \n",
       "8     None     None        None            None         None   \n",
       "9     None     None        None            None         None   \n",
       "\n",
       "            fetched_at  \n",
       "0  2025-09-11T13:40:16  \n",
       "1  2025-09-11T13:40:18  \n",
       "2  2025-09-11T13:40:27  \n",
       "3  2025-09-11T13:40:38  \n",
       "4  2025-09-11T13:40:39  \n",
       "5  2025-09-11T13:40:42  \n",
       "6  2025-09-11T13:40:50  \n",
       "7  2025-09-11T13:40:53  \n",
       "8  2025-09-11T13:40:55  \n",
       "9  2025-09-11T13:41:03  "
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==== Run the crawlers ====\n",
    "with open(\"configs/crawl_config.yaml\", \"r\", encoding=\"utf-8\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "all_records = []\n",
    "\n",
    "# Reddit\n",
    "for sub in cfg[\"sources\"][\"reddit\"][\"subreddits\"]:\n",
    "    try:\n",
    "        recs = crawl_reddit_subreddit(sub, cfg)\n",
    "        all_records.extend(recs)\n",
    "        print(f\"reddit {sub} records:\", len(recs))\n",
    "    except Exception as e:\n",
    "        print(\"reddit error\", sub, e)\n",
    "\n",
    "# News\n",
    "for site in cfg[\"sources\"][\"news_sites\"]:\n",
    "    try:\n",
    "        recs = crawl_news_site(site, cfg)\n",
    "        all_records.extend(recs)\n",
    "        print(f\"news {site['name']} records:\", len(recs))\n",
    "    except Exception as e:\n",
    "        print(\"news error\", site[\"name\"], e)\n",
    "\n",
    "df = to_dataframe(all_records)\n",
    "ts = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
    "out_path = f\"storage/raw/crawl_{ts}.csv\"\n",
    "save_csv(df, out_path)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSV schema & saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial run and output production"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
