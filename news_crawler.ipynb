{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> imports <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, yaml\n",
    "\n",
    "import time, threading, re, random\n",
    "from collections import defaultdict\n",
    "import urllib.parse as urlparse\n",
    "import requests\n",
    "from urllib.robotparser import RobotFileParser\n",
    "import tldextract\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 1.\tCrawler / Scraper <h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining basic settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configs/crawl_config.yaml saved\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"configs\", exist_ok=True)\n",
    "os.makedirs(\"storage/raw\", exist_ok=True)\n",
    "os.makedirs(\"storage/clean\", exist_ok=True)\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "CONFIG = {\n",
    "    \"user_agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36\",\n",
    "    \"default_timeout_sec\": 15,\n",
    "    \"per_host_min_delay_sec\": 2,\n",
    "    \"max_retries\": 3,\n",
    "    \"sources\": {\n",
    "        \"reddit\": {\n",
    "            \"type\": \"reddit_html\",\n",
    "            \"subreddits\": [\"worldnews\", \"news\"],\n",
    "            \"limit_per_sub\": 20\n",
    "        },\n",
    "        \"news_sites\": [\n",
    "            {\n",
    "                \"name\": \"reuters_world\",\n",
    "                \"start_urls\": [\"https://www.reuters.com/world/\"],\n",
    "                \"article_selector_hint\": \"a\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(\"configs/crawl_config.yaml\", \"w\", encoding=\"utf-8\") as f:\n",
    "    yaml.safe_dump(CONFIG, f, allow_unicode=True)\n",
    "print(\"configs/crawl_config.yaml saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the robots.txt and control the rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "_session = requests.Session()\n",
    "# _session.headers.update({\"Accept-Language\": \"en;q=0.9\"})\n",
    "_session.headers.update({\n",
    "    \"Accept-Language\": \"en;q=0.9\",\n",
    "    \"User-Agent\": CONFIG[\"user_agent\"],   # اضافه شود\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "    \"Connection\": \"keep-alive\"\n",
    "})\n",
    "\n",
    "_robot_cache = {}\n",
    "_host_next_time = defaultdict(float)\n",
    "_lock = threading.Lock()\n",
    "\n",
    "def get_robots_parser(base_url, ua):\n",
    "    parsed = urlparse.urlparse(base_url)\n",
    "    robots_url = f\"{parsed.scheme}://{parsed.netloc}/robots.txt\"\n",
    "    if robots_url in _robot_cache:\n",
    "        return _robot_cache[robots_url]\n",
    "    rp = RobotFileParser()\n",
    "    try:\n",
    "        resp = _session.get(robots_url, timeout=8)\n",
    "        if resp.status_code == 200:\n",
    "            rp.parse(resp.text.splitlines())\n",
    "            print(\"resp.status_code == 200\")\n",
    "        else:\n",
    "            rp.parse([])\n",
    "            print(\"resp.status_code != 200\")\n",
    "    except Exception:\n",
    "        rp.parse([])\n",
    "    _robot_cache[robots_url] = rp\n",
    "    return rp\n",
    "\n",
    "def host_key(url):\n",
    "    p = urlparse.urlparse(url)\n",
    "    ext = tldextract.extract(p.netloc)\n",
    "    return \".\".join([x for x in [ext.domain, ext.suffix] if x])\n",
    "\n",
    "def rate_limit(url, min_delay_sec):\n",
    "    hk = host_key(url)\n",
    "    with _lock:\n",
    "        now = time.time()\n",
    "        nt = _host_next_time[hk]\n",
    "        wait = nt - now\n",
    "        if wait > 0:\n",
    "            time.sleep(wait)\n",
    "        _host_next_time[hk] = time.time() + min_delay_sec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch function (with Retry & Exponential Backoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch(url, user_agent, timeout=15, max_retries=3, min_delay_sec=2):\n",
    "    rp = get_robots_parser(url, user_agent)\n",
    "    if not rp.can_fetch(user_agent, url):\n",
    "        raise PermissionError(f\"Blocked by robots.txt for {url}\")\n",
    "    rate_limit(url, min_delay_sec)\n",
    "    headers = {\"User-Agent\": user_agent}\n",
    "    attempt = 0\n",
    "    backoff = 1.6\n",
    "    while attempt <= max_retries:\n",
    "        try:\n",
    "            resp = _session.get(url, headers=headers, timeout=timeout)\n",
    "            if 200 <= resp.status_code < 300:\n",
    "                return resp\n",
    "            if resp.status_code in (429, 503):\n",
    "                time.sleep((backoff ** attempt) + random.uniform(0, 0.5))\n",
    "            else:\n",
    "                time.sleep(0.6)\n",
    "        except requests.RequestException:\n",
    "            time.sleep((backoff ** attempt) + 0.4)\n",
    "        attempt += 1\n",
    "    raise TimeoutError(f\"Fetch failed after retries for {url}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML Crawler for Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_reddit_listing(html, base_url):\n",
    "#     soup = BeautifulSoup(html, \"lxml\")\n",
    "#     items = []\n",
    "#     # Generic selector for post container\n",
    "#     for post in soup.select(\"[data-testid='post-container']\"):\n",
    "#         title_el = post.select_one(\"h3\")\n",
    "#         if not title_el:\n",
    "#             continue\n",
    "#         title = title_el.get_text(strip=True)\n",
    "#         link_el = post.find(\"a\", href=True)\n",
    "#         url = urlparse.urljoin(base_url, link_el[\"href\"]) if link_el else None\n",
    "\n",
    "#         items.append({\n",
    "#             \"source_type\": \"reddit\",\n",
    "#             \"source_name\": base_url,\n",
    "#             \"subreddit\": base_url.rstrip(\"/\").split(\"/\")[-1],\n",
    "#             \"url\": url,\n",
    "#             \"canonical_url\": url,\n",
    "#             \"title\": title,\n",
    "#             \"text\": None,\n",
    "#             \"author\": None,\n",
    "#             \"published_at\": None,\n",
    "#             \"score\": None,\n",
    "#             \"comments\": None,\n",
    "#             \"fetched_at\": datetime.utcnow().isoformat(timespec=\"seconds\")\n",
    "#         })\n",
    "#     return items\n",
    "\n",
    "# def crawl_reddit_subreddit(sub, cfg):\n",
    "#     ua = cfg[\"user_agent\"]\n",
    "#     limit = cfg[\"sources\"][\"reddit\"][\"limit_per_sub\"]\n",
    "#     url = f\"https://www.reddit.com/r/{sub}/\"\n",
    "#     resp = fetch(\n",
    "#         url, ua, timeout=cfg[\"default_timeout_sec\"], \n",
    "#         max_retries=cfg[\"max_retries\"], \n",
    "#         min_delay_sec=cfg[\"per_host_min_delay_sec\"]\n",
    "#     )\n",
    "#     items = parse_reddit_listing(resp.text, url)\n",
    "#     return items[:limit] if limit else items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import urllib.parse as urlparse\n",
    "\n",
    "UPVOTE_RE = re.compile(r\"(\\d+(?:\\.\\d+)?)([kK])?\\s*upvote\")  # برای استخراج عدد از aria-label\n",
    "\n",
    "def to_int_k(v):\n",
    "    if v is None:\n",
    "        return None\n",
    "    s = str(v).strip().lower()\n",
    "    if s.endswith(\"k\"):\n",
    "        try:\n",
    "            return int(float(s[:-1]) * 1000)\n",
    "        except:\n",
    "            return None\n",
    "    try:\n",
    "        return int(s)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parse_reddit_listing(html, base_url):\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    items = []\n",
    "\n",
    "    for post in soup.select(\"[data-testid='post-container'], div[data-test-id='post-content']\"):\n",
    "        # عنوان\n",
    "        title_el = post.select_one(\"h3\") or post.select_one(\"a[data-click-id='body'] h3\")\n",
    "        if not title_el:\n",
    "            continue\n",
    "        title = title_el.get_text(strip=True)\n",
    "\n",
    "        # لینک پایدار پست\n",
    "        link_el = post.select_one(\"a[data-click-id='body']\") or post.find(\"a\", href=True)\n",
    "        url = urlparse.urljoin(base_url, link_el[\"href\"]) if link_el and link_el.get(\"href\") else None\n",
    "\n",
    "        # نویسنده\n",
    "        author_el = post.select_one(\"a[data-click-id='user']\") or post.select_one(\"a[href^='/user/']\")\n",
    "        author = author_el.get_text(strip=True) if author_el else None\n",
    "\n",
    "        # زمان انتشار\n",
    "        time_el = post.select_one(\"a[data-click-id='timestamp'] time\") or post.find(\"time\")\n",
    "        published_at = time_el.get(\"datetime\") if time_el and time_el.has_attr(\"datetime\") else None\n",
    "\n",
    "        # شمار نظرها\n",
    "        comments_el = post.select_one(\"a[data-click-id='comments']\") or post.find(\"a\", string=re.compile(\"comment\", re.I))\n",
    "        comments = None\n",
    "        if comments_el:\n",
    "            m = re.search(r\"(\\d+(?:\\.\\d+)?[kK]?)\", comments_el.get_text(\" \", strip=True))\n",
    "            if m:\n",
    "                comments = to_int_k(m.group(1))\n",
    "\n",
    "        # امتیاز تقریبی\n",
    "        score = None\n",
    "        aria_up = post.find(attrs={\"aria-label\": re.compile(\"upvote\", re.I)})\n",
    "        if aria_up:\n",
    "            m = UPVOTE_RE.search(aria_up.get(\"aria-label\", \"\"))\n",
    "            if m:\n",
    "                val = float(m.group(1))\n",
    "                if m.group(2):\n",
    "                    val *= 1000\n",
    "                score = int(val)\n",
    "\n",
    "        items.append({\n",
    "            \"source_type\": \"reddit\",\n",
    "            \"source_name\": base_url,\n",
    "            \"subreddit\": base_url.rstrip(\"/\").split(\"/\")[-1],\n",
    "            \"url\": url,\n",
    "            \"canonical_url\": url,  # بعدا در مرحله کاننیکال اصلاح می‌کنیم\n",
    "            \"title\": title,\n",
    "            \"text\": None,\n",
    "            \"author\": author,\n",
    "            \"published_at\": published_at,\n",
    "            \"score\": score,\n",
    "            \"comments\": comments,\n",
    "            \"fetched_at\": datetime.utcnow().isoformat(timespec=\"seconds\")\n",
    "        })\n",
    "    return items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from hashlib import sha1\n",
    "\n",
    "# STRIP_PARAMS = {\"utm_source\",\"utm_medium\",\"utm_campaign\",\"utm_term\",\"utm_content\",\"ref\",\"utm_name\"}\n",
    "\n",
    "# def normalize_url(u):\n",
    "#     if not u:\n",
    "#         return None\n",
    "#     p = urlparse.urlparse(u)\n",
    "#     # حذف پارامترهای رهگیری\n",
    "#     q = urlparse.parse_qsl(p.query, keep_blank_values=True)\n",
    "#     q = [(k,v) for k,v in q if k not in STRIP_PARAMS]\n",
    "#     new_q = urlparse.urlencode(q)\n",
    "#     norm = urlparse.urlunparse((p.scheme, p.netloc, p.path, \"\", new_q, \"\"))\n",
    "#     return norm\n",
    "\n",
    "# def get_canonical_from_html(html, url):\n",
    "#     soup = BeautifulSoup(html, \"lxml\")\n",
    "#     link = soup.find(\"link\", rel=lambda x: x and \"canonical\" in x.lower())\n",
    "#     if link and link.get(\"href\"):\n",
    "#         return urlparse.urljoin(url, link[\"href\"])\n",
    "#     # پشتیبان، اگر داده ساخت یافته وجود داشت\n",
    "#     meta = soup.find(\"meta\", property=\"og:url\")\n",
    "#     if meta and meta.get(\"content\"):\n",
    "#         return urlparse.urljoin(url, meta[\"content\"])\n",
    "#     return None\n",
    "\n",
    "# def dedupe_records(records):\n",
    "#     seen = set()\n",
    "#     unique = []\n",
    "#     for r in records:\n",
    "#         key = normalize_url(r.get(\"canonical_url\") or r.get(\"url\")) or r.get(\"url\")\n",
    "#         if not key:\n",
    "#             continue\n",
    "#         h = sha1(key.encode(\"utf-8\")).hexdigest()\n",
    "#         if h in seen:\n",
    "#             continue\n",
    "#         seen.add(h)\n",
    "#         r[\"canonical_url\"] = key\n",
    "#         unique.append(r)\n",
    "#     return unique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== helpers for canonicalization and meta extraction ====\n",
    "import re\n",
    "from hashlib import sha1\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse as urlparse\n",
    "\n",
    "STRIP_PARAMS = {\"utm_source\",\"utm_medium\",\"utm_campaign\",\"utm_term\",\"utm_content\",\"ref\",\"utm_name\",\"gclid\",\"fbclid\"}\n",
    "\n",
    "def normalize_url(u: str) -> str | None:\n",
    "    if not u:\n",
    "        return None\n",
    "    p = urlparse.urlparse(u)\n",
    "    if p.scheme not in (\"http\",\"https\"):\n",
    "        return None\n",
    "    q = [(k,v) for k,v in urlparse.parse_qsl(p.query, keep_blank_values=True) if k not in STRIP_PARAMS]\n",
    "    new_q = urlparse.urlencode(q)\n",
    "    return urlparse.urlunparse((p.scheme, p.netloc, p.path, \"\", new_q, \"\"))\n",
    "\n",
    "def get_canonical_from_html(html: str, url: str) -> str | None:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    link = soup.find(\"link\", rel=lambda x: x and \"canonical\" in x.lower())\n",
    "    if link and link.get(\"href\"):\n",
    "        return urlparse.urljoin(url, link[\"href\"])\n",
    "    meta = soup.find(\"meta\", property=\"og:url\")\n",
    "    if meta and meta.get(\"content\"):\n",
    "        return urlparse.urljoin(url, meta[\"content\"])\n",
    "    return None\n",
    "\n",
    "def extract_article_meta(html: str, url: str):\n",
    "    \"\"\"برگشت می دهد title و author و published_at و canonical\"\"\"\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    # title\n",
    "    title = None\n",
    "    for sel in [\n",
    "        \"meta[property='og:title']\",\n",
    "        \"meta[name='twitter:title']\",\n",
    "        \"title\"\n",
    "    ]:\n",
    "        el = soup.select_one(sel)\n",
    "        if el:\n",
    "            title = el.get(\"content\") if el.has_attr(\"content\") else el.get_text(strip=True)\n",
    "        if title:\n",
    "            break\n",
    "\n",
    "    # author\n",
    "    author = None\n",
    "    for sel in [\n",
    "        \"meta[name='author']\",\n",
    "        \"meta[property='article:author']\",\n",
    "        \"a[rel='author']\"\n",
    "    ]:\n",
    "        el = soup.select_one(sel)\n",
    "        if el:\n",
    "            author = el.get(\"content\") if el.has_attr(\"content\") else el.get_text(strip=True)\n",
    "        if author:\n",
    "            break\n",
    "\n",
    "    # published time\n",
    "    published_at = None\n",
    "    for sel in [\n",
    "        \"meta[property='article:published_time']\",\n",
    "        \"meta[name='pubdate']\",\n",
    "        \"time[datetime]\"\n",
    "    ]:\n",
    "        el = soup.select_one(sel)\n",
    "        if el:\n",
    "            published_at = el.get(\"content\") if el.has_attr(\"content\") else el.get(\"datetime\")\n",
    "        if published_at:\n",
    "            break\n",
    "\n",
    "    # canonical\n",
    "    can = get_canonical_from_html(html, url) or url\n",
    "    can = normalize_url(can)\n",
    "\n",
    "    return title, author, published_at, can\n",
    "\n",
    "def dedupe_records(records):\n",
    "    seen = set()\n",
    "    unique = []\n",
    "    for r in records:\n",
    "        key = normalize_url(r.get(\"canonical_url\") or r.get(\"url\")) or r.get(\"url\")\n",
    "        if not key:\n",
    "            continue\n",
    "        h = sha1(key.encode(\"utf-8\")).hexdigest()\n",
    "        if h in seen:\n",
    "            continue\n",
    "        seen.add(h)\n",
    "        r[\"canonical_url\"] = key\n",
    "        unique.append(r)\n",
    "    return unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_canonical_for_some(items, cfg, take=10):\n",
    "    ua = cfg[\"user_agent\"]\n",
    "    for i, r in enumerate(items[:take]):\n",
    "        if not r.get(\"url\"):\n",
    "            continue\n",
    "        try:\n",
    "            resp = fetch(r[\"url\"], ua, timeout=cfg[\"default_timeout_sec\"], \n",
    "                         max_retries=cfg[\"max_retries\"], \n",
    "                         min_delay_sec=cfg[\"per_host_min_delay_sec\"])\n",
    "            can = get_canonical_from_html(resp.text, r[\"url\"])\n",
    "            if can:\n",
    "                r[\"canonical_url\"] = normalize_url(can)\n",
    "            else:\n",
    "                r[\"canonical_url\"] = normalize_url(r[\"url\"])\n",
    "        except Exception:\n",
    "            r[\"canonical_url\"] = normalize_url(r[\"url\"])\n",
    "    # بقیه موارد حداقل نرمالایز شوند\n",
    "    for r in items[take:]:\n",
    "        r[\"canonical_url\"] = normalize_url(r.get(\"url\"))\n",
    "    return items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_reddit_subreddit(sub, cfg):\n",
    "    ua = cfg[\"user_agent\"]\n",
    "    limit = cfg[\"sources\"][\"reddit\"][\"limit_per_sub\"]\n",
    "    url = f\"https://www.reddit.com/r/{sub}/\"\n",
    "    resp = fetch(url, ua, timeout=cfg[\"default_timeout_sec\"], \n",
    "                 max_retries=cfg[\"max_retries\"], \n",
    "                 min_delay_sec=cfg[\"per_host_min_delay_sec\"])\n",
    "    items = parse_reddit_listing(resp.text, url)\n",
    "\n",
    "    # خواندن کاننیکال برای چند مورد اول\n",
    "    items = enrich_canonical_for_some(items, cfg, take=10)\n",
    "    # حذف تکراری\n",
    "    items = dedupe_records(items)\n",
    "\n",
    "    return items[:limit] if limit else items\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from readability import Document\n",
    "\n",
    "# def extract_article_text(html):\n",
    "#     try:\n",
    "#         doc = Document(html)\n",
    "#         content_html = doc.summary()\n",
    "#         soup = BeautifulSoup(content_html, \"lxml\")\n",
    "#         text = soup.get_text(\"\\n\", strip=True)\n",
    "#         if len(text) < 200:\n",
    "#             # fallback\n",
    "#             soup_full = BeautifulSoup(html, \"lxml\")\n",
    "#             paras = [p.get_text(\" \", strip=True) for p in soup_full.select(\"p\")]\n",
    "#             text = \"\\n\".join(paras[:60])\n",
    "#         return text\n",
    "#     except Exception:\n",
    "#         soup = BeautifulSoup(html, \"lxml\")\n",
    "#         paras = [p.get_text(\" \", strip=True) for p in soup.select(\"p\")]\n",
    "#         return \"\\n\".join(paras[:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== article text extraction (بدون تغییر اساسی) ====\n",
    "from readability import Document\n",
    "\n",
    "def extract_article_text(html):\n",
    "    try:\n",
    "        doc = Document(html)\n",
    "        content_html = doc.summary()\n",
    "        soup = BeautifulSoup(content_html, \"lxml\")\n",
    "        text = soup.get_text(\"\\n\", strip=True)\n",
    "        if len(text) < 200:\n",
    "            soup_full = BeautifulSoup(html, \"lxml\")\n",
    "            paras = [p.get_text(\" \", strip=True) for p in soup_full.select(\"p\")]\n",
    "            text = \"\\n\".join(paras[:60])\n",
    "        return text\n",
    "    except Exception:\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        paras = [p.get_text(\" \", strip=True) for p in soup.select(\"p\")]\n",
    "        return \"\\n\".join(paras[:60])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_listing_find_links(html, base_url, selector_hint=\"a\"):\n",
    "#     soup = BeautifulSoup(html, \"lxml\")\n",
    "#     links = []\n",
    "#     for a in soup.select(selector_hint):\n",
    "#         href = a.get(\"href\")\n",
    "#         if not href:\n",
    "#             continue\n",
    "#         full = urlparse.urljoin(base_url, href)\n",
    "#         links.append(full)\n",
    "#     # حذف تکراری\n",
    "#     return list(dict.fromkeys(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== smarter link discovery on listing pages ====\n",
    "def parse_listing_find_links(html, base_url, selector_hint=\"a\"):\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    base = urlparse.urlparse(base_url).netloc\n",
    "    links = []\n",
    "    for a in soup.select(selector_hint):\n",
    "        href = a.get(\"href\")\n",
    "        if not href:\n",
    "            continue\n",
    "        full = urlparse.urljoin(base_url, href)\n",
    "        norm = normalize_url(full)\n",
    "        if not norm:\n",
    "            continue\n",
    "        # only same-site pages, skip anchors and mailto and javascript\n",
    "        net = urlparse.urlparse(norm).netloc\n",
    "        if net != base:\n",
    "            continue\n",
    "        if norm.endswith(\"#\"):\n",
    "            norm = norm[:-1]\n",
    "        links.append(norm)\n",
    "    # unique while keeping order\n",
    "    return list(dict.fromkeys(links))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def crawl_news_site(entry, cfg):\n",
    "#     ua = cfg[\"user_agent\"]\n",
    "#     start_urls = entry[\"start_urls\"]\n",
    "#     selector_hint = entry.get(\"article_selector_hint\", \"a\")\n",
    "#     records = []\n",
    "#     for su in start_urls:\n",
    "#         resp = fetch(su, ua, timeout=cfg[\"default_timeout_sec\"], max_retries=cfg[\"max_retries\"], min_delay_sec=cfg[\"per_host_min_delay_sec\"])\n",
    "#         links = parse_listing_find_links(resp.text, su, selector_hint)\n",
    "#         for lk in links[:30]:\n",
    "#             try:\n",
    "#                 art = fetch(lk, ua, timeout=cfg[\"default_timeout_sec\"], max_retries=cfg[\"max_retries\"], min_delay_sec=cfg[\"per_host_min_delay_sec\"])\n",
    "#                 text = extract_article_text(art.text)\n",
    "#                 records.append({\n",
    "#                     \"source_type\": \"news\",\n",
    "#                     \"source_name\": entry[\"name\"],\n",
    "#                     \"subreddit\": None,\n",
    "#                     \"url\": lk,\n",
    "#                     \"canonical_url\": lk,\n",
    "#                     \"title\": None,\n",
    "#                     \"text\": text,\n",
    "#                     \"author\": None,\n",
    "#                     \"published_at\": None,\n",
    "#                     \"score\": None,\n",
    "#                     \"comments\": None,\n",
    "#                     \"fetched_at\": datetime.utcnow().isoformat(timespec=\"seconds\")\n",
    "#                 })\n",
    "#             except Exception as e:\n",
    "#                 print(\"error on\", lk, e)\n",
    "#     return records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== crawl a news site with meta, canonical and dedupe ====\n",
    "from datetime import datetime\n",
    "\n",
    "def crawl_news_site(entry, cfg, max_article_per_listing=30, min_text_len=300):\n",
    "    ua = cfg[\"user_agent\"]\n",
    "    start_urls = entry[\"start_urls\"]\n",
    "    selector_hint = entry.get(\"article_selector_hint\", \"a\")\n",
    "    records = []\n",
    "    seen = set()  # dedupe on canonical\n",
    "\n",
    "    for su in start_urls:\n",
    "        resp = fetch(su, ua,\n",
    "                     timeout=cfg[\"default_timeout_sec\"],\n",
    "                     max_retries=cfg[\"max_retries\"],\n",
    "                     min_delay_sec=cfg[\"per_host_min_delay_sec\"])\n",
    "        links = parse_listing_find_links(resp.text, su, selector_hint)\n",
    "\n",
    "        for lk in links[:max_article_per_listing]:\n",
    "            try:\n",
    "                art = fetch(lk, ua,\n",
    "                            timeout=cfg[\"default_timeout_sec\"],\n",
    "                            max_retries=cfg[\"max_retries\"],\n",
    "                            min_delay_sec=cfg[\"per_host_min_delay_sec\"])\n",
    "                text = extract_article_text(art.text) or \"\"\n",
    "                if len(text) < min_text_len:\n",
    "                    continue  # likely not an article\n",
    "\n",
    "                title, author, published_at, canonical = extract_article_meta(art.text, lk)\n",
    "                canonical = canonical or normalize_url(lk)\n",
    "                if not canonical:\n",
    "                    continue\n",
    "                h = sha1(canonical.encode(\"utf-8\")).hexdigest()\n",
    "                if h in seen:\n",
    "                    continue\n",
    "                seen.add(h)\n",
    "\n",
    "                records.append({\n",
    "                    \"source_type\": \"news\",\n",
    "                    \"source_name\": entry[\"name\"],\n",
    "                    \"subreddit\": None,\n",
    "                    \"url\": lk,\n",
    "                    \"canonical_url\": canonical,\n",
    "                    \"title\": title,\n",
    "                    \"text\": text,\n",
    "                    \"author\": author,\n",
    "                    \"published_at\": published_at,\n",
    "                    \"score\": None,\n",
    "                    \"comments\": None,\n",
    "                    \"fetched_at\": datetime.utcnow().isoformat(timespec=\"seconds\")\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(\"error on\", lk, e)\n",
    "    return records\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSV schema & saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_SCHEMA = [\n",
    "    \"source_type\", \"source_name\", \"subreddit\", \"url\", \"canonical_url\",\n",
    "    \"title\", \"text\", \"author\", \"published_at\", \"score\", \"comments\",\n",
    "    \"language\", \"token_count\", \"predicted_label\", \"label_scores\", \"fetched_at\"\n",
    "]\n",
    "\n",
    "def to_dataframe(records):\n",
    "    df = pd.DataFrame(records)\n",
    "    for col in CSV_SCHEMA:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "    return df[CSV_SCHEMA]\n",
    "\n",
    "def save_csv(df, path):\n",
    "    df.to_csv(path, index=False, encoding=\"utf-8\")\n",
    "    print(\"saved\", path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial run and output production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resp.status_code == 200\n",
      "reddit error worldnews Blocked by robots.txt for https://www.reddit.com/r/worldnews/\n",
      "reddit error news Blocked by robots.txt for https://www.reddit.com/r/news/\n",
      "resp.status_code == 200\n",
      "news reuters_world records: 30\n",
      "saved storage/raw/crawl_20250911_001620.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_type</th>\n",
       "      <th>source_name</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>url</th>\n",
       "      <th>canonical_url</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>published_at</th>\n",
       "      <th>score</th>\n",
       "      <th>comments</th>\n",
       "      <th>language</th>\n",
       "      <th>token_count</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>label_scores</th>\n",
       "      <th>fetched_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>news</td>\n",
       "      <td>reuters_world</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.reuters.com/world/#main-content</td>\n",
       "      <td>https://www.reuters.com/world/#main-content</td>\n",
       "      <td>None</td>\n",
       "      <td>category\\n·\\nSeptember 10, 2025\\n·  ago\\nPolan...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-09-11T00:15:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>news</td>\n",
       "      <td>reuters_world</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.reuters.com/differentiator/</td>\n",
       "      <td>https://www.reuters.com/differentiator/</td>\n",
       "      <td>None</td>\n",
       "      <td>Reuters and LSEG: an exclusive partnership\\nLS...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-09-11T00:15:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>news</td>\n",
       "      <td>reuters_world</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.reuters.com/</td>\n",
       "      <td>https://www.reuters.com/</td>\n",
       "      <td>None</td>\n",
       "      <td>ago\\nOracle shares surged about 43% to a recor...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-09-11T00:15:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>news</td>\n",
       "      <td>reuters_world</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.reuters.com/world/</td>\n",
       "      <td>https://www.reuters.com/world/</td>\n",
       "      <td>None</td>\n",
       "      <td>category\\n·\\nSeptember 10, 2025\\n·  ago\\nPolan...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-09-11T00:15:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>news</td>\n",
       "      <td>reuters_world</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.reuters.com/world/africa/</td>\n",
       "      <td>https://www.reuters.com/world/africa/</td>\n",
       "      <td>None</td>\n",
       "      <td>Africa\\ncategory\\n·\\nSeptember 10, 2025\\n·  ag...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-09-11T00:15:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>news</td>\n",
       "      <td>reuters_world</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.reuters.com/world/americas/</td>\n",
       "      <td>https://www.reuters.com/world/americas/</td>\n",
       "      <td>None</td>\n",
       "      <td>category\\n·\\nSeptember 10, 2025\\n·  ago\\nChina...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-09-11T00:15:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>news</td>\n",
       "      <td>reuters_world</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.reuters.com/world/asia-pacific/</td>\n",
       "      <td>https://www.reuters.com/world/asia-pacific/</td>\n",
       "      <td>None</td>\n",
       "      <td>Asia Pacific\\ncategory\\n·\\nSeptember 10, 2025\\...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-09-11T00:15:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>news</td>\n",
       "      <td>reuters_world</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.reuters.com/world/china/</td>\n",
       "      <td>https://www.reuters.com/world/china/</td>\n",
       "      <td>None</td>\n",
       "      <td>category\\n·\\nSeptember 10, 2025\\n·  ago\\nChina...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-09-11T00:15:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>news</td>\n",
       "      <td>reuters_world</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.reuters.com/world/europe/</td>\n",
       "      <td>https://www.reuters.com/world/europe/</td>\n",
       "      <td>None</td>\n",
       "      <td>United Kingdom\\ncategory\\n·\\nSeptember 10, 202...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-09-11T00:15:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>news</td>\n",
       "      <td>reuters_world</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.reuters.com/world/india/</td>\n",
       "      <td>https://www.reuters.com/world/india/</td>\n",
       "      <td>None</td>\n",
       "      <td>India\\ncategory\\n·\\nSeptember 10, 2025\\n·  ago...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-09-11T00:15:39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source_type    source_name subreddit  \\\n",
       "0        news  reuters_world      None   \n",
       "1        news  reuters_world      None   \n",
       "2        news  reuters_world      None   \n",
       "3        news  reuters_world      None   \n",
       "4        news  reuters_world      None   \n",
       "5        news  reuters_world      None   \n",
       "6        news  reuters_world      None   \n",
       "7        news  reuters_world      None   \n",
       "8        news  reuters_world      None   \n",
       "9        news  reuters_world      None   \n",
       "\n",
       "                                           url  \\\n",
       "0  https://www.reuters.com/world/#main-content   \n",
       "1      https://www.reuters.com/differentiator/   \n",
       "2                     https://www.reuters.com/   \n",
       "3               https://www.reuters.com/world/   \n",
       "4        https://www.reuters.com/world/africa/   \n",
       "5      https://www.reuters.com/world/americas/   \n",
       "6  https://www.reuters.com/world/asia-pacific/   \n",
       "7         https://www.reuters.com/world/china/   \n",
       "8        https://www.reuters.com/world/europe/   \n",
       "9         https://www.reuters.com/world/india/   \n",
       "\n",
       "                                 canonical_url title  \\\n",
       "0  https://www.reuters.com/world/#main-content  None   \n",
       "1      https://www.reuters.com/differentiator/  None   \n",
       "2                     https://www.reuters.com/  None   \n",
       "3               https://www.reuters.com/world/  None   \n",
       "4        https://www.reuters.com/world/africa/  None   \n",
       "5      https://www.reuters.com/world/americas/  None   \n",
       "6  https://www.reuters.com/world/asia-pacific/  None   \n",
       "7         https://www.reuters.com/world/china/  None   \n",
       "8        https://www.reuters.com/world/europe/  None   \n",
       "9         https://www.reuters.com/world/india/  None   \n",
       "\n",
       "                                                text author published_at  \\\n",
       "0  category\\n·\\nSeptember 10, 2025\\n·  ago\\nPolan...   None         None   \n",
       "1  Reuters and LSEG: an exclusive partnership\\nLS...   None         None   \n",
       "2  ago\\nOracle shares surged about 43% to a recor...   None         None   \n",
       "3  category\\n·\\nSeptember 10, 2025\\n·  ago\\nPolan...   None         None   \n",
       "4  Africa\\ncategory\\n·\\nSeptember 10, 2025\\n·  ag...   None         None   \n",
       "5  category\\n·\\nSeptember 10, 2025\\n·  ago\\nChina...   None         None   \n",
       "6  Asia Pacific\\ncategory\\n·\\nSeptember 10, 2025\\...   None         None   \n",
       "7  category\\n·\\nSeptember 10, 2025\\n·  ago\\nChina...   None         None   \n",
       "8  United Kingdom\\ncategory\\n·\\nSeptember 10, 202...   None         None   \n",
       "9  India\\ncategory\\n·\\nSeptember 10, 2025\\n·  ago...   None         None   \n",
       "\n",
       "  score comments language token_count predicted_label label_scores  \\\n",
       "0  None     None     None        None            None         None   \n",
       "1  None     None     None        None            None         None   \n",
       "2  None     None     None        None            None         None   \n",
       "3  None     None     None        None            None         None   \n",
       "4  None     None     None        None            None         None   \n",
       "5  None     None     None        None            None         None   \n",
       "6  None     None     None        None            None         None   \n",
       "7  None     None     None        None            None         None   \n",
       "8  None     None     None        None            None         None   \n",
       "9  None     None     None        None            None         None   \n",
       "\n",
       "            fetched_at  \n",
       "0  2025-09-11T00:15:20  \n",
       "1  2025-09-11T00:15:23  \n",
       "2  2025-09-11T00:15:25  \n",
       "3  2025-09-11T00:15:26  \n",
       "4  2025-09-11T00:15:28  \n",
       "5  2025-09-11T00:15:31  \n",
       "6  2025-09-11T00:15:32  \n",
       "7  2025-09-11T00:15:34  \n",
       "8  2025-09-11T00:15:37  \n",
       "9  2025-09-11T00:15:39  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "with open(\"configs/crawl_config.yaml\", \"r\", encoding=\"utf-8\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "all_records = []\n",
    "\n",
    "# Reddit\n",
    "for sub in cfg[\"sources\"][\"reddit\"][\"subreddits\"]:\n",
    "    try:\n",
    "        recs = crawl_reddit_subreddit(sub, cfg)\n",
    "        all_records.extend(recs)\n",
    "        print(f\"reddit {sub} records:\", len(recs))\n",
    "    except Exception as e:\n",
    "        print(\"reddit error\", sub, e)\n",
    "\n",
    "# News\n",
    "for site in cfg[\"sources\"][\"news_sites\"]:\n",
    "    try:\n",
    "        recs = crawl_news_site(site, cfg)\n",
    "        all_records.extend(recs)\n",
    "        print(f\"news {site['name']} records:\", len(recs))\n",
    "    except Exception as e:\n",
    "        print(\"news error\", site[\"name\"], e)\n",
    "\n",
    "df = to_dataframe(all_records)\n",
    "ts = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
    "out_path = f\"storage/raw/crawl_{ts}.csv\"\n",
    "save_csv(df, out_path)\n",
    "df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
