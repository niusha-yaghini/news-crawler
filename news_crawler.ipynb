{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> imports <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\niush\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\niush\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "c:\\Users\\niush\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "C:\\Users\\niush\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import os, yaml\n",
    "\n",
    "import time, threading, re, random\n",
    "from collections import defaultdict\n",
    "import urllib.parse as urlparse\n",
    "import requests\n",
    "from urllib.robotparser import RobotFileParser\n",
    "import tldextract\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import urllib.parse as urlparse\n",
    "import pandas as pd\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 1.\tCrawler / Scraper <h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining basic settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configs/crawl_config.yaml saved\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"configs\", exist_ok=True)\n",
    "os.makedirs(\"storage/raw\", exist_ok=True)\n",
    "os.makedirs(\"storage/clean\", exist_ok=True)\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "CONFIG = {\n",
    "    \"user_agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36\",\n",
    "    \"default_timeout_sec\": 15,\n",
    "    \"per_host_min_delay_sec\": 2,\n",
    "    \"max_retries\": 3,\n",
    "    \"sources\": {\n",
    "        \"reddit\": {\n",
    "            \"type\": \"reddit_html\",\n",
    "            \"subreddits\": [\"worldnews\", \"news\"],\n",
    "            \"limit_per_sub\": 20\n",
    "        },\n",
    "        \"news_sites\": [\n",
    "            {\n",
    "                \"name\": \"reuters_world\",\n",
    "                \"start_urls\": [\"https://www.reuters.com/world/\"],\n",
    "                \"article_selector_hint\": \"a\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(\"configs/crawl_config.yaml\", \"w\", encoding=\"utf-8\") as f:\n",
    "    yaml.safe_dump(CONFIG, f, allow_unicode=True)\n",
    "print(\"configs/crawl_config.yaml saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the robots.txt and control the rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "_session = requests.Session()\n",
    "_session.headers.update({\"Accept-Language\": \"en;q=0.9\"})\n",
    "\n",
    "_robot_cache = {}\n",
    "_host_next_time = defaultdict(float)\n",
    "_lock = threading.Lock()\n",
    "\n",
    "def get_robots_parser(base_url, ua):\n",
    "    parsed = urlparse.urlparse(base_url)\n",
    "    robots_url = f\"{parsed.scheme}://{parsed.netloc}/robots.txt\"\n",
    "    if robots_url in _robot_cache:\n",
    "        return _robot_cache[robots_url]\n",
    "    rp = RobotFileParser()\n",
    "    try:\n",
    "        resp = _session.get(robots_url, timeout=8)\n",
    "        if resp.status_code == 200:\n",
    "            rp.parse(resp.text.splitlines())\n",
    "            print(\"resp.status_code == 200\")\n",
    "        else:\n",
    "            rp.parse([])\n",
    "            print(\"resp.status_code != 200\")\n",
    "    except Exception:\n",
    "        rp.parse([])\n",
    "    _robot_cache[robots_url] = rp\n",
    "    return rp\n",
    "\n",
    "def host_key(url):\n",
    "    p = urlparse.urlparse(url)\n",
    "    ext = tldextract.extract(p.netloc)\n",
    "    return \".\".join([x for x in [ext.domain, ext.suffix] if x])\n",
    "\n",
    "def rate_limit(url, min_delay_sec):\n",
    "    hk = host_key(url)\n",
    "    with _lock:\n",
    "        now = time.time()\n",
    "        nt = _host_next_time[hk]\n",
    "        wait = nt - now\n",
    "        if wait > 0:\n",
    "            time.sleep(wait)\n",
    "        _host_next_time[hk] = time.time() + min_delay_sec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch function (with Retry & Exponential Backoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch(url, user_agent, timeout=15, max_retries=3, min_delay_sec=2):\n",
    "    rp = get_robots_parser(url, user_agent)\n",
    "    if not rp.can_fetch(user_agent, url):\n",
    "        raise PermissionError(f\"Blocked by robots.txt for {url}\")\n",
    "    rate_limit(url, min_delay_sec)\n",
    "    headers = {\"User-Agent\": user_agent}\n",
    "    attempt = 0\n",
    "    backoff = 1.6\n",
    "    while attempt <= max_retries:\n",
    "        try:\n",
    "            resp = _session.get(url, headers=headers, timeout=timeout)\n",
    "            if 200 <= resp.status_code < 300:\n",
    "                return resp\n",
    "            if resp.status_code in (429, 503):\n",
    "                time.sleep((backoff ** attempt) + random.uniform(0, 0.5))\n",
    "            else:\n",
    "                time.sleep(0.6)\n",
    "        except requests.RequestException:\n",
    "            time.sleep((backoff ** attempt) + 0.4)\n",
    "        attempt += 1\n",
    "    raise TimeoutError(f\"Fetch failed after retries for {url}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML Crawler for Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_reddit_listing(html, base_url):\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    items = []\n",
    "    # Generic selector for post container\n",
    "    for post in soup.select(\"[data-testid='post-container']\"):\n",
    "        title_el = post.select_one(\"h3\")\n",
    "        if not title_el:\n",
    "            continue\n",
    "        title = title_el.get_text(strip=True)\n",
    "        link_el = post.find(\"a\", href=True)\n",
    "        url = urlparse.urljoin(base_url, link_el[\"href\"]) if link_el else None\n",
    "\n",
    "        items.append({\n",
    "            \"source_type\": \"reddit\",\n",
    "            \"source_name\": base_url,\n",
    "            \"subreddit\": base_url.rstrip(\"/\").split(\"/\")[-1],\n",
    "            \"url\": url,\n",
    "            \"canonical_url\": url,\n",
    "            \"title\": title,\n",
    "            \"text\": None,\n",
    "            \"author\": None,\n",
    "            \"published_at\": None,\n",
    "            \"score\": None,\n",
    "            \"comments\": None,\n",
    "            \"fetched_at\": datetime.utcnow().isoformat(timespec=\"seconds\")\n",
    "        })\n",
    "    return items\n",
    "\n",
    "def crawl_reddit_subreddit(sub, cfg):\n",
    "    ua = cfg[\"user_agent\"]\n",
    "    limit = cfg[\"sources\"][\"reddit\"][\"limit_per_sub\"]\n",
    "    url = f\"https://www.reddit.com/r/{sub}/\"\n",
    "    resp = fetch(\n",
    "        url, ua, timeout=cfg[\"default_timeout_sec\"], \n",
    "        max_retries=cfg[\"max_retries\"], \n",
    "        min_delay_sec=cfg[\"per_host_min_delay_sec\"]\n",
    "    )\n",
    "    items = parse_reddit_listing(resp.text, url)\n",
    "    return items[:limit] if limit else items\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
