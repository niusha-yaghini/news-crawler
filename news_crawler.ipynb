{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> imports <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, threading, random, re, urllib.parse as urlparse, yaml, requests, pandas as pd\n",
    "from collections import defaultdict\n",
    "from urllib.robotparser import RobotFileParser\n",
    "from bs4 import BeautifulSoup\n",
    "from hashlib import sha1\n",
    "import tldextract\n",
    "from datetime import datetime\n",
    "from readability import Document\n",
    "import feedparser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 1.\tCrawler / Scraper <h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining basic settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs(\"configs\", exist_ok=True)\n",
    "# os.makedirs(\"storage/raw\", exist_ok=True)\n",
    "# os.makedirs(\"storage/clean\", exist_ok=True)\n",
    "# os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "# CONFIG = {\n",
    "#     \"user_agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36\",\n",
    "#     \"default_timeout_sec\": 15,\n",
    "#     \"per_host_min_delay_sec\": 2,\n",
    "#     \"max_retries\": 3,\n",
    "#     \"sources\": {\n",
    "#         \"reddit\": {\n",
    "#             \"type\": \"reddit_html\",\n",
    "#             \"subreddits\": [\"worldnews\", \"news\"],\n",
    "#             \"limit_per_sub\": 0\n",
    "#         },\n",
    "#         \"news_sites\": [\n",
    "#             {\n",
    "#                 \"name\": \"reuters_world\",\n",
    "#                 \"start_urls\": [\"https://www.reuters.com/world/\"],\n",
    "#                 \"article_selector_hint\": \"a\"\n",
    "#             }\n",
    "#         ]\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# with open(\"configs/crawl_config.yaml\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     yaml.safe_dump(CONFIG, f, allow_unicode=True)\n",
    "# print(\"configs/crawl_config.yaml saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configs/crawl_config.yaml saved\n"
     ]
    }
   ],
   "source": [
    "import os, yaml\n",
    "\n",
    "os.makedirs(\"configs\", exist_ok=True)\n",
    "os.makedirs(\"storage/raw\", exist_ok=True)\n",
    "os.makedirs(\"storage/clean\", exist_ok=True)\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "CONFIG = {\n",
    "    \"user_agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36\",\n",
    "    \"default_timeout_sec\": 15,\n",
    "    \"per_host_min_delay_sec\": 2,\n",
    "    \"max_retries\": 3,\n",
    "    \"sources\": {\n",
    "        \"reddit\": {\n",
    "            \"type\": \"reddit_html\",\n",
    "            \"subreddits\": [\n",
    "                \"worldnews\",\n",
    "                \"news\",\n",
    "                \"politics\",\n",
    "                \"technology\",\n",
    "                \"science\",\n",
    "                \"environment\",\n",
    "                \"economics\"\n",
    "            ],\n",
    "            \"limit_per_sub\": 50\n",
    "        },\n",
    "        \"npr_news\": {\n",
    "            \"type\": \"news_html\",\n",
    "            \"start_urls\": [\"https://www.npr.org/sections/world/\"],\n",
    "            \"article_selector_hint\": \"a\",\n",
    "            \"include_patterns\": [\"/story/\", \"/202\", \"/sections/world/\"],\n",
    "            \"limit_per_section\": 50\n",
    "        },\n",
    "        \"news_sites\": [\n",
    "            {\n",
    "                \"name\": \"ap_world\",\n",
    "                \"start_urls\": [\"https://apnews.com/hub/world-news\"],\n",
    "                \"article_selector_hint\": \"a\"\n",
    "            }\n",
    "        ]\n",
    "        # \"news_sites\": [\n",
    "        #     {\n",
    "        #         \"name\": \"reuters_world\",\n",
    "        #         \"start_urls\": [\"https://www.reuters.com/world/\"],\n",
    "        #         \"article_selector_hint\": \"a\"\n",
    "        #     }\n",
    "        # ]\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(\"configs/crawl_config.yaml\", \"w\", encoding=\"utf-8\") as f:\n",
    "    yaml.safe_dump(CONFIG, f, allow_unicode=True)\n",
    "print(\"configs/crawl_config.yaml saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the robots.txt and control the rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "_session = requests.Session()\n",
    "# _session.headers.update({\"Accept-Language\": \"en;q=0.9\"})\n",
    "_session.headers.update({\n",
    "    \"Accept-Language\": \"en;q=0.9\",\n",
    "    \"User-Agent\": CONFIG[\"user_agent\"],\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "    \"Connection\": \"keep-alive\"\n",
    "})\n",
    "\n",
    "# ==== robots.txt handling and rate limit ====\n",
    "_robot_cache = {}\n",
    "_host_next_time = defaultdict(float)\n",
    "_lock = threading.Lock()\n",
    "\n",
    "def get_robots_parser(base_url, ua):\n",
    "    parsed = urlparse.urlparse(base_url)\n",
    "    robots_url = f\"{parsed.scheme}://{parsed.netloc}/robots.txt\"\n",
    "    if robots_url in _robot_cache:\n",
    "        return _robot_cache[robots_url]\n",
    "    rp = RobotFileParser()\n",
    "    try:\n",
    "        resp = _session.get(robots_url, timeout=8)\n",
    "        if resp.status_code == 200:\n",
    "            rp.parse(resp.text.splitlines())\n",
    "        else:\n",
    "            rp.parse([])\n",
    "    except Exception:\n",
    "        rp.parse([])\n",
    "    _robot_cache[robots_url] = rp\n",
    "    return rp\n",
    "\n",
    "def host_key(url):\n",
    "    p = urlparse.urlparse(url)\n",
    "    ext = tldextract.extract(p.netloc)\n",
    "    return \".\".join([x for x in [ext.domain, ext.suffix] if x])\n",
    "\n",
    "def rate_limit(url, min_delay_sec):\n",
    "    hk = host_key(url)\n",
    "    with _lock:\n",
    "        now = time.time()\n",
    "        nt = _host_next_time[hk]\n",
    "        wait = nt - now\n",
    "        if wait > 0:\n",
    "            time.sleep(wait)\n",
    "        _host_next_time[hk] = time.time() + min_delay_sec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch function (with Retry & Exponential Backoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch(url, user_agent, timeout=15, max_retries=3, min_delay_sec=2):\n",
    "    rp = get_robots_parser(url, user_agent)\n",
    "    if not rp.can_fetch(user_agent, url):\n",
    "        raise PermissionError(f\"Blocked by robots.txt for {url}\")\n",
    "\n",
    "    rate_limit(url, min_delay_sec)\n",
    "\n",
    "    headers = {\"User-Agent\": user_agent}\n",
    "    attempt = 0\n",
    "    backoff = 1.6\n",
    "\n",
    "    while attempt <= max_retries:\n",
    "        try:\n",
    "            resp = _session.get(url, headers=headers, timeout=timeout, allow_redirects=True)\n",
    "\n",
    "            if 200 <= resp.status_code < 300:\n",
    "                return resp\n",
    "\n",
    "            # محدودیت نرخ و خطای موقت\n",
    "            if resp.status_code in (429, 502, 503, 504):\n",
    "                sleep_sec = (backoff ** attempt) + random.uniform(0, 0.5)\n",
    "                time.sleep(sleep_sec)\n",
    "\n",
    "            # عدم دسترسی یا محدودیت قانونی\n",
    "            elif resp.status_code in (401, 403, 451):\n",
    "                break\n",
    "\n",
    "            else:\n",
    "                time.sleep(0.6)\n",
    "\n",
    "        except requests.RequestException:\n",
    "            time.sleep((backoff ** attempt) + 0.4)\n",
    "\n",
    "        attempt += 1\n",
    "\n",
    "    raise TimeoutError(f\"Fetch failed after retries for {url}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML Crawler for Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "UPVOTE_RE = re.compile(r\"(\\d+(?:\\.\\d+)?)([kK])?\\s*upvote\")\n",
    "\n",
    "def to_int_k(v):\n",
    "    if v is None:\n",
    "        return None\n",
    "    s = str(v).strip().lower()\n",
    "    if s.endswith(\"k\"):\n",
    "        try:\n",
    "            return int(float(s[:-1]) * 1000)\n",
    "        except:\n",
    "            return None\n",
    "    try:\n",
    "        return int(s)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parse_reddit_listing(html, base_url):\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    items = []\n",
    "    for post in soup.select(\"[data-testid='post-container'], div[data-test-id='post-content']\"):\n",
    "        title_el = post.select_one(\"h3\") or post.select_one(\"a[data-click-id='body'] h3\")\n",
    "        if not title_el:\n",
    "            continue\n",
    "        title = title_el.get_text(strip=True)\n",
    "        # link_el = post.select_one(\"a[data-click-id='body']\") or post.find(\"a\", href=True)\n",
    "        link_el = post.select_one(\"a[data-click-id='body'][href]\") or post.select_one(\"a[href^='/r/']\")\n",
    "\n",
    "        url = urlparse.urljoin(base_url, link_el[\"href\"]) if link_el and link_el.get(\"href\") else None\n",
    "        if not url:\n",
    "            continue\n",
    "        \n",
    "        author_el = post.select_one(\"a[data-click-id='user']\") or post.select_one(\"a[href^='/user/']\")\n",
    "        author = author_el.get_text(strip=True) if author_el else None\n",
    "        time_el = post.select_one(\"a[data-click-id='timestamp'] time\") or post.find(\"time\")\n",
    "        published_at = time_el.get(\"datetime\") if time_el and time_el.has_attr(\"datetime\") else None\n",
    "        comments_el = post.select_one(\"a[data-click-id='comments']\") or post.find(\"a\", string=re.compile(\"comment\", re.I))\n",
    "        comments = None\n",
    "        if comments_el:\n",
    "            m = re.search(r\"(\\d+(?:\\.\\d+)?[kK]?)\", comments_el.get_text(\" \", strip=True))\n",
    "            if m:\n",
    "                comments = to_int_k(m.group(1))\n",
    "        score = None\n",
    "        aria_up = post.find(attrs={\"aria-label\": re.compile(\"upvote\", re.I)})\n",
    "        if aria_up:\n",
    "            m = UPVOTE_RE.search(aria_up.get(\"aria-label\", \"\"))\n",
    "            if m:\n",
    "                val = float(m.group(1))\n",
    "                if m.group(2):\n",
    "                    val *= 1000\n",
    "                score = int(val)\n",
    "        items.append({\n",
    "            \"source_type\": \"reddit\",\n",
    "            \"source_name\": base_url,\n",
    "            \"subreddit\": base_url.rstrip(\"/\").split(\"/\")[-1],\n",
    "            \"url\": url,\n",
    "            \"canonical_url\": url,\n",
    "            \"title\": title,\n",
    "            \"text\": None,\n",
    "            \"author\": author,\n",
    "            \"published_at\": published_at,\n",
    "            \"score\": score,\n",
    "            \"comments\": comments,\n",
    "            \"fetched_at\": datetime.utcnow().isoformat(timespec=\"seconds\")\n",
    "        })\n",
    "    return items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "STRIP_PARAMS = {\"utm_source\",\"utm_medium\",\"utm_campaign\",\"utm_term\",\"utm_content\",\"ref\",\"utm_name\",\"gclid\",\"fbclid\"}\n",
    "\n",
    "def normalize_url(u: str) -> str | None:\n",
    "    if not u:\n",
    "        return None\n",
    "    p = urlparse.urlparse(u)\n",
    "    if p.scheme not in (\"http\",\"https\"):\n",
    "        return None\n",
    "    q = [(k,v) for k,v in urlparse.parse_qsl(p.query, keep_blank_values=True) if k not in STRIP_PARAMS]\n",
    "    new_q = urlparse.urlencode(q)\n",
    "    return urlparse.urlunparse((p.scheme, p.netloc, p.path, \"\", new_q, \"\"))\n",
    "\n",
    "def dedupe_records(records):\n",
    "    seen = set()\n",
    "    unique = []\n",
    "    for r in records:\n",
    "        key = normalize_url(r.get(\"canonical_url\") or r.get(\"url\")) or r.get(\"url\")\n",
    "        if not key:\n",
    "            continue\n",
    "        h = sha1(key.encode(\"utf-8\")).hexdigest()\n",
    "        if h in seen:\n",
    "            continue\n",
    "        seen.add(h)\n",
    "        r[\"canonical_url\"] = key\n",
    "        unique.append(r)\n",
    "    return unique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reddit_rss_url(sub: str) -> str:\n",
    "    # RSS پایدار روی www.reddit.com بهتر جواب می‌دهد\n",
    "    return f\"https://www.reddit.com/r/{sub}/.rss\"\n",
    "\n",
    "def crawl_reddit_via_rss(sub, cfg, limit=None):\n",
    "    rss = reddit_rss_url(sub)\n",
    "    ua = cfg[\"user_agent\"]\n",
    "\n",
    "    # robots برای خود مسیر RSS چک شود\n",
    "    rp = get_robots_parser(rss, ua)\n",
    "    if not rp.can_fetch(ua, rss):\n",
    "        print(f\"robots disallows RSS for {rss}\")\n",
    "        return []\n",
    "\n",
    "    # با Session خودمان fetch کنیم تا UA درست ارسال شود\n",
    "    resp = fetch(rss, ua,\n",
    "                 timeout=cfg[\"default_timeout_sec\"],\n",
    "                 max_retries=cfg[\"max_retries\"],\n",
    "                 min_delay_sec=cfg[\"per_host_min_delay_sec\"])\n",
    "\n",
    "    print(\"rss status:\", resp.status_code, \"bytes:\", len(resp.text))\n",
    "\n",
    "    d = feedparser.parse(resp.text)\n",
    "    print(\"rss entries:\", len(d.get(\"entries\", [])))\n",
    "    items = []\n",
    "    now = datetime.utcnow().isoformat(timespec=\"seconds\")\n",
    "    for e in d.get(\"entries\", [])[: (limit or 50)]:\n",
    "        link = e.get(\"link\")\n",
    "        items.append({\n",
    "            \"source_type\": \"reddit\",\n",
    "            \"source_name\": rss,\n",
    "            \"subreddit\": sub,\n",
    "            \"url\": link,\n",
    "            \"canonical_url\": normalize_url(link) if link else link,\n",
    "            \"title\": e.get(\"title\"),\n",
    "            \"text\": None,\n",
    "            \"author\": e.get(\"author\") if \"author\" in e else None,\n",
    "            \"published_at\": e.get(\"published\") if \"published\" in e else None,\n",
    "            \"score\": None,\n",
    "            \"comments\": None,\n",
    "            \"fetched_at\": now\n",
    "        })\n",
    "    return dedupe_records(items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_reddit_subreddit(sub, cfg):\n",
    "    ua = cfg[\"user_agent\"]\n",
    "    limit = cfg[\"sources\"][\"reddit\"][\"limit_per_sub\"]\n",
    "    html_url = f\"https://old.reddit.com/r/{sub}/\"\n",
    "    rp = get_robots_parser(html_url, ua)\n",
    "    if not rp.can_fetch(ua, html_url):\n",
    "        print(f\"robots disallows HTML for {html_url}, switching to RSS\")\n",
    "        return crawl_reddit_via_rss(sub, cfg, limit)\n",
    "    # اگر HTML مجاز بود همان مسیر قبلی\n",
    "    resp = fetch(html_url, ua,\n",
    "                 timeout=cfg[\"default_timeout_sec\"],\n",
    "                 max_retries=cfg[\"max_retries\"],\n",
    "                 min_delay_sec=cfg[\"per_host_min_delay_sec\"])\n",
    "    items = parse_reddit_listing(resp.text, html_url)\n",
    "    items = dedupe_records(items)\n",
    "    return items[:limit] if limit else items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Article text extraction ====\n",
    "def extract_article_text(html):\n",
    "    try:\n",
    "        doc = Document(html)\n",
    "        content_html = doc.summary()\n",
    "        soup = BeautifulSoup(content_html, \"lxml\")\n",
    "        text = soup.get_text(\"\\n\", strip=True)\n",
    "        if len(text) < 200:\n",
    "            soup_full = BeautifulSoup(html, \"lxml\")\n",
    "            paras = [p.get_text(\" \", strip=True) for p in soup_full.select(\"p\")]\n",
    "            text = \"\\n\".join(paras[:60])\n",
    "        return text\n",
    "    except Exception:\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        paras = [p.get_text(\" \", strip=True) for p in soup.select(\"p\")]\n",
    "        return \"\\n\".join(paras[:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Link discovery for news sites ====\n",
    "def parse_listing_find_links(html, base_url, selector_hint=\"a\"):\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    base = urlparse.urlparse(base_url).netloc\n",
    "    links = []\n",
    "    for a in soup.select(selector_hint):\n",
    "        href = a.get(\"href\")\n",
    "        if not href:\n",
    "            continue\n",
    "        full = urlparse.urljoin(base_url, href)\n",
    "        norm = normalize_url(full)\n",
    "        if not norm:\n",
    "            continue\n",
    "        net = urlparse.urlparse(norm).netloc\n",
    "        if net != base:\n",
    "            continue\n",
    "        if norm.endswith(\"#\"):\n",
    "            norm = norm[:-1]\n",
    "        links.append(norm)\n",
    "    return list(dict.fromkeys(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Consent page detection ====\n",
    "def looks_like_consent_page(html: str) -> bool:\n",
    "    s = html.lower()\n",
    "    return (\"consent\" in s) or (\"gdpr\" in s) or (\"privacy preferences\" in s) or (\"iab\" in s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Extract article meta (title, author, published_at, canonical) ====\n",
    "def extract_article_meta(html: str, url: str):\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    title = None\n",
    "    for sel in [\"meta[property='og:title']\", \"meta[name='twitter:title']\", \"title\"]:\n",
    "        el = soup.select_one(sel)\n",
    "        if el:\n",
    "            title = el.get(\"content\") if el.has_attr(\"content\") else el.get_text(strip=True)\n",
    "        if title:\n",
    "            break\n",
    "    author = None\n",
    "    for sel in [\"meta[name='author']\", \"meta[property='article:author']\", \"a[rel='author']\"]:\n",
    "        el = soup.select_one(sel)\n",
    "        if el:\n",
    "            author = el.get(\"content\") if el.has_attr(\"content\") else el.get_text(strip=True)\n",
    "        if author:\n",
    "            break\n",
    "    published_at = None\n",
    "    for sel in [\"meta[property='article:published_time']\", \"meta[name='pubdate']\", \"time[datetime]\"]:\n",
    "        el = soup.select_one(sel)\n",
    "        if el:\n",
    "            published_at = el.get(\"content\") if el.has_attr(\"content\") else el.get(\"datetime\")\n",
    "        if published_at:\n",
    "            break\n",
    "    can = soup.find(\"link\", rel=lambda x: x and \"canonical\" in x.lower())\n",
    "    canonical = urlparse.urljoin(url, can[\"href\"]) if can and can.get(\"href\") else url\n",
    "    canonical = normalize_url(canonical)\n",
    "    return title, author, published_at, canonical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Crawl news site ====\n",
    "def crawl_news_site(entry, cfg, max_article_per_listing=30, min_text_len=300):\n",
    "    ua = cfg[\"user_agent\"]\n",
    "    start_urls = entry[\"start_urls\"]\n",
    "    selector_hint = entry.get(\"article_selector_hint\", \"a\")\n",
    "    records = []\n",
    "    seen = set()\n",
    "    for su in start_urls:\n",
    "        resp = fetch(su, ua,\n",
    "                     timeout=cfg[\"default_timeout_sec\"],\n",
    "                     max_retries=cfg[\"max_retries\"],\n",
    "                     min_delay_sec=cfg[\"per_host_min_delay_sec\"])\n",
    "        links = parse_listing_find_links(resp.text, su, selector_hint)\n",
    "        for lk in links[:max_article_per_listing]:\n",
    "            try:\n",
    "                art = fetch(lk, ua,\n",
    "                            timeout=cfg[\"default_timeout_sec\"],\n",
    "                            max_retries=cfg[\"max_retries\"],\n",
    "                            min_delay_sec=cfg[\"per_host_min_delay_sec\"])\n",
    "                if looks_like_consent_page(art.text):\n",
    "                    print(\"skip consent page:\", lk)\n",
    "                    continue\n",
    "                text = extract_article_text(art.text) or \"\"\n",
    "                if len(text) < min_text_len:\n",
    "                    continue\n",
    "                title, author, published_at, canonical = extract_article_meta(art.text, lk)\n",
    "                canonical = canonical or normalize_url(lk)\n",
    "                if not canonical:\n",
    "                    continue\n",
    "                h = sha1(canonical.encode(\"utf-8\")).hexdigest()\n",
    "                if h in seen:\n",
    "                    continue\n",
    "                seen.add(h)\n",
    "                records.append({\n",
    "                    \"source_type\": \"news\",\n",
    "                    \"source_name\": entry[\"name\"],\n",
    "                    \"subreddit\": None,\n",
    "                    \"url\": lk,\n",
    "                    \"canonical_url\": canonical,\n",
    "                    \"title\": title,\n",
    "                    \"text\": text,\n",
    "                    \"author\": author,\n",
    "                    \"published_at\": published_at,\n",
    "                    \"score\": None,\n",
    "                    \"comments\": None,\n",
    "                    \"fetched_at\": datetime.utcnow().isoformat(timespec=\"seconds\")\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(\"error on\", lk, e)\n",
    "    return records\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSV Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== CSV Schema and helpers ====\n",
    "CSV_SCHEMA = [\n",
    "    \"source_type\", \"source_name\", \"subreddit\", \"url\", \"canonical_url\",\n",
    "    \"title\", \"text\", \"author\", \"published_at\", \"score\", \"comments\",\n",
    "    \"language\", \"token_count\", \"predicted_label\", \"label_scores\", \"fetched_at\"\n",
    "]\n",
    "\n",
    "def to_dataframe(records):\n",
    "    df = pd.DataFrame(records)\n",
    "    for col in CSV_SCHEMA:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "    return df[CSV_SCHEMA]\n",
    "\n",
    "def save_csv(df, path):\n",
    "    df.to_csv(path, index=False, encoding=\"utf-8\")\n",
    "    print(\"saved\", path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "robots disallows HTML for https://old.reddit.com/r/worldnews/, switching to RSS\n",
      "robots disallows RSS for https://www.reddit.com/r/worldnews/.rss\n",
      "reddit worldnews records: 0\n",
      "robots disallows HTML for https://old.reddit.com/r/news/, switching to RSS\n",
      "robots disallows RSS for https://www.reddit.com/r/news/.rss\n",
      "reddit news records: 0\n",
      "robots disallows HTML for https://old.reddit.com/r/politics/, switching to RSS\n",
      "robots disallows RSS for https://www.reddit.com/r/politics/.rss\n",
      "reddit politics records: 0\n",
      "robots disallows HTML for https://old.reddit.com/r/technology/, switching to RSS\n",
      "robots disallows RSS for https://www.reddit.com/r/technology/.rss\n",
      "reddit technology records: 0\n",
      "robots disallows HTML for https://old.reddit.com/r/science/, switching to RSS\n",
      "robots disallows RSS for https://www.reddit.com/r/science/.rss\n",
      "reddit science records: 0\n",
      "robots disallows HTML for https://old.reddit.com/r/environment/, switching to RSS\n",
      "robots disallows RSS for https://www.reddit.com/r/environment/.rss\n",
      "reddit environment records: 0\n",
      "robots disallows HTML for https://old.reddit.com/r/economics/, switching to RSS\n",
      "robots disallows RSS for https://www.reddit.com/r/economics/.rss\n",
      "reddit economics records: 0\n",
      "skip consent page: https://apnews.com/\n",
      "skip consent page: https://apnews.com/world-news\n",
      "skip consent page: https://apnews.com/hub/israel-hamas-war\n",
      "skip consent page: https://apnews.com/hub/russia-ukraine\n",
      "skip consent page: https://apnews.com/hub/noticias\n",
      "skip consent page: https://apnews.com/hub/china\n",
      "skip consent page: https://apnews.com/hub/asia-pacific\n",
      "skip consent page: https://apnews.com/hub/latin-america\n",
      "skip consent page: https://apnews.com/hub/europe\n",
      "skip consent page: https://apnews.com/hub/africa\n",
      "skip consent page: https://apnews.com/article/nato-poland-russia-ukraine-drones-article4-658921ca98eff77e39345041ba0900a3\n",
      "skip consent page: https://apnews.com/article/uk-mandelson-epstein-fc3f953112ac10108e1109920fd9dca0\n",
      "skip consent page: https://apnews.com/article/prince-harry-king-charles-26a71a950773fb222e6f690c124ff0ff\n",
      "skip consent page: https://apnews.com/newsletters?id=Morning+Wire+Subscribers\n",
      "skip consent page: https://apnews.com/newsletters?id=Afternoon+Wire\n",
      "skip consent page: https://apnews.com/newsletters\n",
      "skip consent page: https://apnews.com/us-news\n",
      "skip consent page: https://apnews.com/hub/immigration\n",
      "skip consent page: https://apnews.com/hub/weather\n",
      "skip consent page: https://apnews.com/education\n",
      "skip consent page: https://apnews.com/us-news/transportation\n",
      "skip consent page: https://apnews.com/hub/abortion\n",
      "skip consent page: https://apnews.com/hub/lgbtq\n",
      "skip consent page: https://apnews.com/hub/obituaries\n",
      "skip consent page: https://apnews.com/article/charlie-kirk-conservative-activist-shot-546165a8151104e0938a5e085be1e8bd\n",
      "skip consent page: https://apnews.com/live/utah-valley-university-charlie-kirk-shooting-updates\n",
      "skip consent page: https://apnews.com/article/charlie-kirk-video-graphic-online-social-media-6cfd4dfde356b960aeea69c01ea3ec34\n",
      "skip consent page: https://apnews.com/quizzes\n",
      "skip consent page: https://apnews.com/politics\n",
      "skip consent page: https://apnews.com/hub/donald-trump\n",
      "news ap_world records: 0\n",
      "saved storage/raw/crawl_20250911_150615.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_type</th>\n",
       "      <th>source_name</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>url</th>\n",
       "      <th>canonical_url</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>published_at</th>\n",
       "      <th>score</th>\n",
       "      <th>comments</th>\n",
       "      <th>language</th>\n",
       "      <th>token_count</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>label_scores</th>\n",
       "      <th>fetched_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [source_type, source_name, subreddit, url, canonical_url, title, text, author, published_at, score, comments, language, token_count, predicted_label, label_scores, fetched_at]\n",
       "Index: []"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==== Run the crawlers ====\n",
    "with open(\"configs/crawl_config.yaml\", \"r\", encoding=\"utf-8\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "all_records = []\n",
    "\n",
    "# Reddit\n",
    "for sub in cfg[\"sources\"][\"reddit\"][\"subreddits\"]:\n",
    "    try:\n",
    "        recs = crawl_reddit_subreddit(sub, cfg)\n",
    "        all_records.extend(recs)\n",
    "        print(f\"reddit {sub} records:\", len(recs))\n",
    "    except Exception as e:\n",
    "        print(\"reddit error\", sub, e)\n",
    "\n",
    "# News\n",
    "for site in cfg[\"sources\"][\"news_sites\"]:\n",
    "    try:\n",
    "        recs = crawl_news_site(site, cfg)\n",
    "        all_records.extend(recs)\n",
    "        print(f\"news {site['name']} records:\", len(recs))\n",
    "    except Exception as e:\n",
    "        print(\"news error\", site[\"name\"], e)\n",
    "\n",
    "df = to_dataframe(all_records)\n",
    "ts = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
    "out_path = f\"storage/raw/crawl_{ts}.csv\"\n",
    "save_csv(df, out_path)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NPR news HTML Crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_npr_listing(html, base_url):\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    items = []\n",
    "    for a in soup.select(\"a[href*='/story/'], a[href*='/sections/world/']\"):\n",
    "        href = a.get(\"href\")\n",
    "        if not href:\n",
    "            continue\n",
    "        url = urlparse.urljoin(base_url, href)\n",
    "        url = normalize_url(url)\n",
    "        if not url:\n",
    "            continue\n",
    "        title = a.get_text(\" \", strip=True) or None\n",
    "        items.append({\n",
    "            \"source_type\": \"news\",\n",
    "            \"source_name\": \"npr_world\",\n",
    "            \"subreddit\": None,\n",
    "            \"url\": url,\n",
    "            \"canonical_url\": url,\n",
    "            \"title\": title,\n",
    "            \"text\": None,\n",
    "            \"author\": None,\n",
    "            \"published_at\": None,\n",
    "            \"score\": None,\n",
    "            \"comments\": None,\n",
    "            \"fetched_at\": datetime.utcnow().isoformat(timespec=\"seconds\")\n",
    "        })\n",
    "    return dedupe_records(items)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSV schema & saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial run and output production"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
